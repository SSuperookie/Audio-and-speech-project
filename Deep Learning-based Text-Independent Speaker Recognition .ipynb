{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mini-project: Speaker Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaker recognition is an important tasks in many applications. The problem of speaker recognition aims at determining the speaker identity in a provided recording and matches it to a pre-recorded database or a pre-extracted speaker-related features. It can be applied in places such as secured biometric verification applications (e.g. banks) and command detection systems (e.g. smart home devices).\n",
    "\n",
    "There are two types of tasks in the general problem of speaker recognition - **speaker identification** and **speaker verification**. Speaker identification maps a given utterance to a speaker feature, and matches it to a set of stored speaker features to identify which speaker is speaking. Speaker verification extracts the same speaker feature, but generates a *yes/no* output presenting whether the speaker is the target speaker we want to hear. We can reformulate the speaker identification problem to a one-to-many mapping problem or a multi-class classification problem (as what we did for the acoustic event detection problem), and the speaker verifcation problem to a one-to-one mapping problem or a binary classification problem.\n",
    "\n",
    "Moreover, there are two types of problem settings in speaker recognition - **text-dependent** and **text-independent** - depending on whether we want to put any constraints on the contents. Text-dependent speaker recognition systems are designed for certain contents, such as waken words and key words in smart home devices (\"Hi Siri/Cortana/Google\"), while text-independent speaker recognition systems do not make any assumptions on the content. Intuitively text-independent systems are harder to work but are more general, but if you want higher recognition accuracy in certain environments, text-dependent systems are often more suitable.\n",
    "\n",
    "In this mini-project we will implement a simplified text-independent speaker recognition system. The problem of speaker recognition is dominated by the advances of neural networks in recent years (like other tasks), so our implementation will still be on the network architectures and the training objectives. We call this the *end-to-end neural speaker recognition system*. If you are interested in the conventional methods for speaker recognition, I'll put some links at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pipeline Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard text-independent speaker recognition pipeline contains an **embedding extraction module** and an **embedding scoring module**. The embedding extraction module takes an utterance as input and generates an utterance-level embedding (speaker feature), and the embedding scoring module estimates a score (which is typically a probability or a similarity measure) by comparing the embedding from a given utterance (which is the query utterance) to the set of pre-extracted embeddings for all available speakers (a dictionary or a speaker embedding database). To calculate the speaker embedding database, the system needs a set of **enrollment** utterances from all of the speakers in order to extract the target embeddings, and the enrollments are used as the training data. Well-known systems such as [***d-vector***](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/41939.pdf) and [***x-vector***](https://danielpovey.com/files/2018_icassp_xvectors.pdf) all follow this pipeline.\n",
    "![](https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/28aafb36-9ce9-498d-8c52-be5983e1073f/dec0d528-18a8-435b-95fc-20d2a5da0854/images/screenshot.jpg)\n",
    "\n",
    "During the training of the end-to-end neural speaker recognition system, we have the following example procedure:\n",
    "1. For each of the utterance in the enrollment data, perform a multi-class classification on the target speaker labels (as what we did for the acoustic event detection problem). The only difference is that the output from the second-last layer needs to be normalized.\n",
    "2. Extract the normalized output from the second-last layer in the system (the input to the final output classification layer) as the speaker embedding feature.\n",
    "3. Gather all speaker embeddings from all the enrollment utterances from a certain speaker and use them (or certain statistics of them) as the single speaker embedding for this speaker.\n",
    "\n",
    "During the scoring of a given query utterance, we have the following example procedure:\n",
    "1. Extract the speaker embedding from the query utterance.\n",
    "2. For speaker identification task, compare it with all the pre-extracted speaker embeddings in the training set via certain distance measures (e.g. cosine similarity), determine the speaker identity of the query utterance by certain pre-defined similarity threshold.\n",
    "3. For speaker verification task, only compare it with the target speaker's pre-extracted embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-project we will use a dataset sampled from Librispeech. There are 50 speakers in our data, and each speaker has 10 training utterances, 3 validation utterances and 1 test utterance. All of them are 6 second long. Similar to the previous homework and the acoustic event detection tutorial, let's prepare the data and generate the magnitude spectrogram features. The wave files are saved in the \"SV_data\" folder.\n",
    "\n",
    "Note that here the speaker label for the utterances are denoted by their directory names. Take a look at the data directory arrangements before you start loading and manipulating the wave files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import time\n",
    "import h5py\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: data preparation\n",
    "\n",
    "dir_path = 'all_data/SV_data'  # directory path\n",
    "\n",
    "# walk through the directory, find the files with .wav extension\n",
    "wav_files = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(dir_path):\n",
    "    for file in filenames:\n",
    "        if '.wav' in file:\n",
    "            wav_files.append(dirpath+'/'+file)\n",
    "        \n",
    "wav_files = sorted(wav_files)\n",
    "num_data = len(wav_files)\n",
    "\n",
    "train_audio = []\n",
    "val_audio = []\n",
    "test_audio = []\n",
    "train_target = []\n",
    "val_target = []\n",
    "test_target = []\n",
    "\n",
    "for i in range(50):\n",
    "    y, _ = librosa.load(wav_files[i], sr=16000)\n",
    "    test_audio.append(y)\n",
    "    test_target.append(int(wav_files[i].split('/')[-2][3:]))\n",
    "\n",
    "for i in range(500):\n",
    "    y, _ = librosa.load(wav_files[i+50], sr=16000)\n",
    "    train_audio.append(y) \n",
    "    train_target.append(int(wav_files[i+50].split('/')[-2][3:]))\n",
    "    \n",
    "for i in range(150):\n",
    "    y, _ = librosa.load(wav_files[i+550], sr=16000)\n",
    "    val_audio.append(y)\n",
    "    val_target.append(int(wav_files[i+550].split('/')[-2][3:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same AlexNet architecture as the one used in the acoustic event detection task. The only difference, as we mentioned above, is that the output at the second-last layer has to be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=11, stride=4, padding=2),  # number of input channel is 1 (for image it is 3) \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),  # we make the number of hidden channels smaller in these layers\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((3, 3))  # perform adaptive mean pooling on any size of the input to match the provided size\n",
    "        self.classifier = nn.Sequential(\n",
    "            # nn.Dropout()  no Droupout layers here\n",
    "            nn.Linear(64 * 3 * 3, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout()  no Droupout layers here\n",
    "            nn.Linear(256, 256),\n",
    "            nn.Tanh() # use Tanh instead of ReLU since the output here will be used for the speaker embeddings\n",
    "        )\n",
    "        self.output = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)  # the dimension after adaptive average pooling is (batch, 64, 3, 3)\n",
    "        x = torch.flatten(x, 1)  # average\n",
    "        x = self.classifier(x)  \n",
    "        # normalize before the last layer\n",
    "        embedding = x / (x.pow(2).sum(1) + 1e-6).sqrt().unsqueeze(1)\n",
    "        # output layer\n",
    "        x = self.output(embedding)\n",
    "        return x, embedding\n",
    "    \n",
    "# test it with a sample input\n",
    "model = AlexNet()\n",
    "sample_input = torch.randn(2, 1, 257, 626)  # (batch_size, num_channel, width, height)\n",
    "sample_output, sample_embedding = model(sample_input)\n",
    "print(sample_output.shape, sample_embedding.shape)  # (batch_size, num_classes), (batch_size, embedding_dimension)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the training of the model is identical as what we have done for acoustic event detection. You should still report the overall accuracy with your best model on the 50 **test** utterances. You need to achieve at least 50% overall accuracy on the **validation** set to get the full mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    15/   63 batches | ms/batch 132.07 | CE 3.9131 |\n",
      "| epoch   1 |    30/   63 batches | ms/batch 126.72 | CE 3.9249 |\n",
      "| epoch   1 |    45/   63 batches | ms/batch 126.03 | CE 3.9322 |\n",
      "| epoch   1 |    60/   63 batches | ms/batch 124.37 | CE 3.9273 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time:  7.76s | CE 3.9271 |\n",
      "    | end of validation epoch   1 | time:  0.99s | Accuracy 0.0333 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   2 |    15/   63 batches | ms/batch 123.63 | CE 3.8553 |\n",
      "| epoch   2 |    30/   63 batches | ms/batch 122.84 | CE 3.8486 |\n",
      "| epoch   2 |    45/   63 batches | ms/batch 121.03 | CE 3.8338 |\n",
      "| epoch   2 |    60/   63 batches | ms/batch 121.97 | CE 3.8137 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   2 | time:  7.59s | CE 3.8116 |\n",
      "    | end of validation epoch   2 | time:  0.91s | Accuracy 0.0733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   3 |    15/   63 batches | ms/batch 122.23 | CE 3.6716 |\n",
      "| epoch   3 |    30/   63 batches | ms/batch 130.80 | CE 3.6622 |\n",
      "| epoch   3 |    45/   63 batches | ms/batch 125.50 | CE 3.6540 |\n",
      "| epoch   3 |    60/   63 batches | ms/batch 127.73 | CE 3.6332 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   3 | time:  7.93s | CE 3.6301 |\n",
      "    | end of validation epoch   3 | time:  0.80s | Accuracy 0.0400 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   4 |    15/   63 batches | ms/batch 122.18 | CE 3.4901 |\n",
      "| epoch   4 |    30/   63 batches | ms/batch 119.80 | CE 3.5002 |\n",
      "| epoch   4 |    45/   63 batches | ms/batch 121.66 | CE 3.5052 |\n",
      "| epoch   4 |    60/   63 batches | ms/batch 124.04 | CE 3.4929 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   4 | time:  7.75s | CE 3.4880 |\n",
      "    | end of validation epoch   4 | time:  0.99s | Accuracy 0.1400 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   5 |    15/   63 batches | ms/batch 121.17 | CE 3.3990 |\n",
      "| epoch   5 |    30/   63 batches | ms/batch 121.07 | CE 3.3713 |\n",
      "| epoch   5 |    45/   63 batches | ms/batch 119.89 | CE 3.3652 |\n",
      "| epoch   5 |    60/   63 batches | ms/batch 120.12 | CE 3.3599 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   5 | time:  7.53s | CE 3.3599 |\n",
      "    | end of validation epoch   5 | time:  0.96s | Accuracy 0.1667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   6 |    15/   63 batches | ms/batch 129.34 | CE 3.2994 |\n",
      "| epoch   6 |    30/   63 batches | ms/batch 119.58 | CE 3.2716 |\n",
      "| epoch   6 |    45/   63 batches | ms/batch 118.58 | CE 3.2545 |\n",
      "| epoch   6 |    60/   63 batches | ms/batch 116.76 | CE 3.2465 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   6 | time:  7.26s | CE 3.2527 |\n",
      "    | end of validation epoch   6 | time:  0.80s | Accuracy 0.1667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   7 |    15/   63 batches | ms/batch 111.71 | CE 3.1270 |\n",
      "| epoch   7 |    30/   63 batches | ms/batch 114.33 | CE 3.1444 |\n",
      "| epoch   7 |    45/   63 batches | ms/batch 111.36 | CE 3.1374 |\n",
      "| epoch   7 |    60/   63 batches | ms/batch 115.00 | CE 3.1272 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   7 | time:  7.19s | CE 3.1244 |\n",
      "    | end of validation epoch   7 | time:  1.02s | Accuracy 0.2400 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   8 |    15/   63 batches | ms/batch 123.92 | CE 3.0730 |\n",
      "| epoch   8 |    30/   63 batches | ms/batch 122.53 | CE 3.0504 |\n",
      "| epoch   8 |    45/   63 batches | ms/batch 123.22 | CE 3.0381 |\n",
      "| epoch   8 |    60/   63 batches | ms/batch 119.75 | CE 3.0211 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   8 | time:  7.46s | CE 3.0136 |\n",
      "    | end of validation epoch   8 | time:  0.86s | Accuracy 0.1733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   9 |    15/   63 batches | ms/batch 121.07 | CE 2.9422 |\n",
      "| epoch   9 |    30/   63 batches | ms/batch 118.29 | CE 2.9097 |\n",
      "| epoch   9 |    45/   63 batches | ms/batch 116.39 | CE 2.9140 |\n",
      "| epoch   9 |    60/   63 batches | ms/batch 117.09 | CE 2.9057 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   9 | time:  7.32s | CE 2.9057 |\n",
      "    | end of validation epoch   9 | time:  0.83s | Accuracy 0.2600 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  10 |    15/   63 batches | ms/batch 113.46 | CE 2.8118 |\n",
      "| epoch  10 |    30/   63 batches | ms/batch 115.19 | CE 2.8175 |\n",
      "| epoch  10 |    45/   63 batches | ms/batch 117.28 | CE 2.8096 |\n",
      "| epoch  10 |    60/   63 batches | ms/batch 116.66 | CE 2.8029 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  10 | time:  7.31s | CE 2.7995 |\n",
      "    | end of validation epoch  10 | time:  0.93s | Accuracy 0.3667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  11 |    15/   63 batches | ms/batch 105.34 | CE 2.6689 |\n",
      "| epoch  11 |    30/   63 batches | ms/batch 109.17 | CE 2.6747 |\n",
      "| epoch  11 |    45/   63 batches | ms/batch 113.54 | CE 2.6667 |\n",
      "| epoch  11 |    60/   63 batches | ms/batch 116.23 | CE 2.6834 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  11 | time:  7.30s | CE 2.6825 |\n",
      "    | end of validation epoch  11 | time:  0.84s | Accuracy 0.2867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  12 |    15/   63 batches | ms/batch 112.89 | CE 2.6351 |\n",
      "| epoch  12 |    30/   63 batches | ms/batch 112.10 | CE 2.5950 |\n",
      "| epoch  12 |    45/   63 batches | ms/batch 114.36 | CE 2.5772 |\n",
      "| epoch  12 |    60/   63 batches | ms/batch 114.60 | CE 2.5739 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  12 | time:  7.17s | CE 2.5660 |\n",
      "    | end of validation epoch  12 | time:  0.88s | Accuracy 0.3533 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  13 |    15/   63 batches | ms/batch 119.98 | CE 2.4670 |\n",
      "| epoch  13 |    30/   63 batches | ms/batch 118.50 | CE 2.4447 |\n",
      "| epoch  13 |    45/   63 batches | ms/batch 119.71 | CE 2.4391 |\n",
      "| epoch  13 |    60/   63 batches | ms/batch 116.81 | CE 2.4360 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  13 | time:  7.33s | CE 2.4360 |\n",
      "    | end of validation epoch  13 | time:  0.84s | Accuracy 0.3667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  14 |    15/   63 batches | ms/batch 116.12 | CE 2.4404 |\n",
      "| epoch  14 |    30/   63 batches | ms/batch 115.85 | CE 2.3615 |\n",
      "| epoch  14 |    45/   63 batches | ms/batch 116.46 | CE 2.3315 |\n",
      "| epoch  14 |    60/   63 batches | ms/batch 117.29 | CE 2.3208 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  14 | time:  7.33s | CE 2.3179 |\n",
      "    | end of validation epoch  14 | time:  0.93s | Accuracy 0.4667 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  15 |    15/   63 batches | ms/batch 121.27 | CE 2.2054 |\n",
      "| epoch  15 |    30/   63 batches | ms/batch 121.82 | CE 2.2159 |\n",
      "| epoch  15 |    45/   63 batches | ms/batch 123.18 | CE 2.2214 |\n",
      "| epoch  15 |    60/   63 batches | ms/batch 123.51 | CE 2.2170 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  15 | time:  7.74s | CE 2.2177 |\n",
      "    | end of validation epoch  15 | time:  0.97s | Accuracy 0.4333 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  16 |    15/   63 batches | ms/batch 120.62 | CE 2.1202 |\n",
      "| epoch  16 |    30/   63 batches | ms/batch 126.51 | CE 2.1156 |\n",
      "| epoch  16 |    45/   63 batches | ms/batch 122.88 | CE 2.1248 |\n",
      "| epoch  16 |    60/   63 batches | ms/batch 121.75 | CE 2.1068 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  16 | time:  7.66s | CE 2.1020 |\n",
      "    | end of validation epoch  16 | time:  1.00s | Accuracy 0.3733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  17 |    15/   63 batches | ms/batch 114.90 | CE 2.0590 |\n",
      "| epoch  17 |    30/   63 batches | ms/batch 119.80 | CE 2.0158 |\n",
      "| epoch  17 |    45/   63 batches | ms/batch 119.50 | CE 1.9948 |\n",
      "| epoch  17 |    60/   63 batches | ms/batch 119.40 | CE 1.9551 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  17 | time:  7.49s | CE 1.9537 |\n",
      "    | end of validation epoch  17 | time:  0.83s | Accuracy 0.5067 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  18 |    15/   63 batches | ms/batch 123.66 | CE 1.8120 |\n",
      "| epoch  18 |    30/   63 batches | ms/batch 116.94 | CE 1.7995 |\n",
      "| epoch  18 |    45/   63 batches | ms/batch 112.88 | CE 1.8082 |\n",
      "| epoch  18 |    60/   63 batches | ms/batch 113.61 | CE 1.8056 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  18 | time:  7.15s | CE 1.8117 |\n",
      "    | end of validation epoch  18 | time:  0.88s | Accuracy 0.5000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  19 |    15/   63 batches | ms/batch 114.47 | CE 1.7777 |\n",
      "| epoch  19 |    30/   63 batches | ms/batch 119.37 | CE 1.7390 |\n",
      "| epoch  19 |    45/   63 batches | ms/batch 123.96 | CE 1.7417 |\n",
      "| epoch  19 |    60/   63 batches | ms/batch 122.83 | CE 1.7122 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  19 | time:  7.66s | CE 1.7048 |\n",
      "    | end of validation epoch  19 | time:  0.81s | Accuracy 0.5333 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  20 |    15/   63 batches | ms/batch 106.30 | CE 1.5953 |\n",
      "| epoch  20 |    30/   63 batches | ms/batch 109.82 | CE 1.5875 |\n",
      "| epoch  20 |    45/   63 batches | ms/batch 110.35 | CE 1.5863 |\n",
      "| epoch  20 |    60/   63 batches | ms/batch 110.41 | CE 1.5895 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  20 | time:  6.91s | CE 1.5897 |\n",
      "    | end of validation epoch  20 | time:  0.80s | Accuracy 0.4733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  21 |    15/   63 batches | ms/batch 109.01 | CE 1.4668 |\n",
      "| epoch  21 |    30/   63 batches | ms/batch 108.35 | CE 1.4741 |\n",
      "| epoch  21 |    45/   63 batches | ms/batch 109.23 | CE 1.4820 |\n",
      "| epoch  21 |    60/   63 batches | ms/batch 109.40 | CE 1.4844 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  21 | time:  6.85s | CE 1.4822 |\n",
      "    | end of validation epoch  21 | time:  0.91s | Accuracy 0.5600 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  22 |    15/   63 batches | ms/batch 131.39 | CE 1.4510 |\n",
      "| epoch  22 |    30/   63 batches | ms/batch 122.54 | CE 1.3899 |\n",
      "| epoch  22 |    45/   63 batches | ms/batch 118.05 | CE 1.3882 |\n",
      "| epoch  22 |    60/   63 batches | ms/batch 117.27 | CE 1.3814 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  22 | time:  7.30s | CE 1.3777 |\n",
      "    | end of validation epoch  22 | time:  0.86s | Accuracy 0.4933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  23 |    15/   63 batches | ms/batch 116.22 | CE 1.3625 |\n",
      "| epoch  23 |    30/   63 batches | ms/batch 114.15 | CE 1.3111 |\n",
      "| epoch  23 |    45/   63 batches | ms/batch 111.98 | CE 1.2825 |\n",
      "| epoch  23 |    60/   63 batches | ms/batch 111.21 | CE 1.2603 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  23 | time:  6.94s | CE 1.2611 |\n",
      "    | end of validation epoch  23 | time:  0.91s | Accuracy 0.5600 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  24 |    15/   63 batches | ms/batch 107.99 | CE 1.2025 |\n",
      "| epoch  24 |    30/   63 batches | ms/batch 107.42 | CE 1.1989 |\n",
      "| epoch  24 |    45/   63 batches | ms/batch 107.11 | CE 1.1849 |\n",
      "| epoch  24 |    60/   63 batches | ms/batch 109.04 | CE 1.1743 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  24 | time:  6.83s | CE 1.1693 |\n",
      "    | end of validation epoch  24 | time:  0.82s | Accuracy 0.5400 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  25 |    15/   63 batches | ms/batch 110.27 | CE 1.1125 |\n",
      "| epoch  25 |    30/   63 batches | ms/batch 114.60 | CE 1.0925 |\n",
      "| epoch  25 |    45/   63 batches | ms/batch 113.28 | CE 1.0789 |\n",
      "| epoch  25 |    60/   63 batches | ms/batch 114.90 | CE 1.0783 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  25 | time:  7.18s | CE 1.0789 |\n",
      "    | end of validation epoch  25 | time:  0.88s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  26 |    15/   63 batches | ms/batch 109.01 | CE 1.0133 |\n",
      "| epoch  26 |    30/   63 batches | ms/batch 110.22 | CE 1.0232 |\n",
      "| epoch  26 |    45/   63 batches | ms/batch 113.14 | CE 1.0095 |\n",
      "| epoch  26 |    60/   63 batches | ms/batch 114.50 | CE 0.9871 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  26 | time:  7.21s | CE 0.9907 |\n",
      "    | end of validation epoch  26 | time:  0.83s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  27 |    15/   63 batches | ms/batch 119.65 | CE 0.9092 |\n",
      "| epoch  27 |    30/   63 batches | ms/batch 119.00 | CE 0.9179 |\n",
      "| epoch  27 |    45/   63 batches | ms/batch 118.02 | CE 0.9209 |\n",
      "| epoch  27 |    60/   63 batches | ms/batch 114.93 | CE 0.9161 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  27 | time:  7.15s | CE 0.9107 |\n",
      "    | end of validation epoch  27 | time:  0.78s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  28 |    15/   63 batches | ms/batch 107.19 | CE 0.8488 |\n",
      "| epoch  28 |    30/   63 batches | ms/batch 108.15 | CE 0.8258 |\n",
      "| epoch  28 |    45/   63 batches | ms/batch 106.82 | CE 0.8222 |\n",
      "| epoch  28 |    60/   63 batches | ms/batch 106.54 | CE 0.8135 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  28 | time:  6.67s | CE 0.8138 |\n",
      "    | end of validation epoch  28 | time:  0.82s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  29 |    15/   63 batches | ms/batch 115.20 | CE 0.7625 |\n",
      "| epoch  29 |    30/   63 batches | ms/batch 114.29 | CE 0.7408 |\n",
      "| epoch  29 |    45/   63 batches | ms/batch 118.96 | CE 0.7486 |\n",
      "| epoch  29 |    60/   63 batches | ms/batch 121.28 | CE 0.7350 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  29 | time:  7.57s | CE 0.7360 |\n",
      "    | end of validation epoch  29 | time:  0.84s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  30 |    15/   63 batches | ms/batch 125.09 | CE 0.6850 |\n",
      "| epoch  30 |    30/   63 batches | ms/batch 126.54 | CE 0.6869 |\n",
      "| epoch  30 |    45/   63 batches | ms/batch 123.84 | CE 0.6709 |\n",
      "| epoch  30 |    60/   63 batches | ms/batch 123.62 | CE 0.6683 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  30 | time:  7.74s | CE 0.6647 |\n",
      "    | end of validation epoch  30 | time:  0.90s | Accuracy 0.6733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  31 |    15/   63 batches | ms/batch 119.33 | CE 0.6214 |\n",
      "| epoch  31 |    30/   63 batches | ms/batch 116.45 | CE 0.6210 |\n",
      "| epoch  31 |    45/   63 batches | ms/batch 116.10 | CE 0.6101 |\n",
      "| epoch  31 |    60/   63 batches | ms/batch 116.95 | CE 0.6038 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  31 | time:  7.31s | CE 0.6051 |\n",
      "    | end of validation epoch  31 | time:  0.84s | Accuracy 0.6267 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  32 |    15/   63 batches | ms/batch 121.45 | CE 0.6397 |\n",
      "| epoch  32 |    30/   63 batches | ms/batch 120.42 | CE 0.6186 |\n",
      "| epoch  32 |    45/   63 batches | ms/batch 120.75 | CE 0.5901 |\n",
      "| epoch  32 |    60/   63 batches | ms/batch 124.60 | CE 0.5763 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  32 | time:  7.76s | CE 0.5723 |\n",
      "    | end of validation epoch  32 | time:  0.86s | Accuracy 0.6067 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  33 |    15/   63 batches | ms/batch 123.64 | CE 0.4991 |\n",
      "| epoch  33 |    30/   63 batches | ms/batch 118.71 | CE 0.5043 |\n",
      "| epoch  33 |    45/   63 batches | ms/batch 118.88 | CE 0.4983 |\n",
      "| epoch  33 |    60/   63 batches | ms/batch 118.25 | CE 0.4927 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  33 | time:  7.41s | CE 0.4898 |\n",
      "    | end of validation epoch  33 | time:  0.84s | Accuracy 0.6067 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  34 |    15/   63 batches | ms/batch 121.83 | CE 0.4827 |\n",
      "| epoch  34 |    30/   63 batches | ms/batch 124.80 | CE 0.4440 |\n",
      "| epoch  34 |    45/   63 batches | ms/batch 124.38 | CE 0.4405 |\n",
      "| epoch  34 |    60/   63 batches | ms/batch 121.95 | CE 0.4379 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  34 | time:  7.59s | CE 0.4387 |\n",
      "    | end of validation epoch  34 | time:  0.89s | Accuracy 0.5733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  35 |    15/   63 batches | ms/batch 119.14 | CE 0.3960 |\n",
      "| epoch  35 |    30/   63 batches | ms/batch 117.09 | CE 0.3858 |\n",
      "| epoch  35 |    45/   63 batches | ms/batch 115.22 | CE 0.3926 |\n",
      "| epoch  35 |    60/   63 batches | ms/batch 116.06 | CE 0.3895 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  35 | time:  7.28s | CE 0.3929 |\n",
      "    | end of validation epoch  35 | time:  0.82s | Accuracy 0.6200 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  36 |    15/   63 batches | ms/batch 111.93 | CE 0.3858 |\n",
      "| epoch  36 |    30/   63 batches | ms/batch 111.18 | CE 0.3636 |\n",
      "| epoch  36 |    45/   63 batches | ms/batch 111.46 | CE 0.3602 |\n",
      "| epoch  36 |    60/   63 batches | ms/batch 113.99 | CE 0.3552 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  36 | time:  7.19s | CE 0.3565 |\n",
      "    | end of validation epoch  36 | time:  1.03s | Accuracy 0.6133 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  37 |    15/   63 batches | ms/batch 117.13 | CE 0.3466 |\n",
      "| epoch  37 |    30/   63 batches | ms/batch 127.27 | CE 0.3272 |\n",
      "| epoch  37 |    45/   63 batches | ms/batch 131.52 | CE 0.3188 |\n",
      "| epoch  37 |    60/   63 batches | ms/batch 130.24 | CE 0.3131 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  37 | time:  8.20s | CE 0.3119 |\n",
      "    | end of validation epoch  37 | time:  0.93s | Accuracy 0.6400 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  38 |    15/   63 batches | ms/batch 115.69 | CE 0.2810 |\n",
      "| epoch  38 |    30/   63 batches | ms/batch 117.67 | CE 0.2824 |\n",
      "| epoch  38 |    45/   63 batches | ms/batch 115.45 | CE 0.2814 |\n",
      "| epoch  38 |    60/   63 batches | ms/batch 116.72 | CE 0.2793 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  38 | time:  7.33s | CE 0.2786 |\n",
      "    | end of validation epoch  38 | time:  0.87s | Accuracy 0.6333 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  39 |    15/   63 batches | ms/batch 126.58 | CE 0.2590 |\n",
      "| epoch  39 |    30/   63 batches | ms/batch 126.24 | CE 0.2601 |\n",
      "| epoch  39 |    45/   63 batches | ms/batch 124.04 | CE 0.2513 |\n",
      "| epoch  39 |    60/   63 batches | ms/batch 123.98 | CE 0.2518 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  39 | time:  7.73s | CE 0.2499 |\n",
      "    | end of validation epoch  39 | time:  0.87s | Accuracy 0.6333 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  40 |    15/   63 batches | ms/batch 125.98 | CE 0.2330 |\n",
      "| epoch  40 |    30/   63 batches | ms/batch 127.93 | CE 0.2264 |\n",
      "| epoch  40 |    45/   63 batches | ms/batch 130.65 | CE 0.2216 |\n",
      "| epoch  40 |    60/   63 batches | ms/batch 128.14 | CE 0.2264 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  40 | time:  8.01s | CE 0.2256 |\n",
      "    | end of validation epoch  40 | time:  0.83s | Accuracy 0.6400 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  41 |    15/   63 batches | ms/batch 118.01 | CE 0.2083 |\n",
      "| epoch  41 |    30/   63 batches | ms/batch 119.80 | CE 0.2082 |\n",
      "| epoch  41 |    45/   63 batches | ms/batch 122.05 | CE 0.2067 |\n",
      "| epoch  41 |    60/   63 batches | ms/batch 124.67 | CE 0.2038 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  41 | time:  7.83s | CE 0.2052 |\n",
      "    | end of validation epoch  41 | time:  0.82s | Accuracy 0.6467 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  42 |    15/   63 batches | ms/batch 134.97 | CE 0.1938 |\n",
      "| epoch  42 |    30/   63 batches | ms/batch 125.38 | CE 0.1891 |\n",
      "| epoch  42 |    45/   63 batches | ms/batch 119.71 | CE 0.1889 |\n",
      "| epoch  42 |    60/   63 batches | ms/batch 116.75 | CE 0.1862 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  42 | time:  7.29s | CE 0.1861 |\n",
      "    | end of validation epoch  42 | time:  0.78s | Accuracy 0.6467 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  43 |    15/   63 batches | ms/batch 108.44 | CE 0.1746 |\n",
      "| epoch  43 |    30/   63 batches | ms/batch 107.93 | CE 0.1753 |\n",
      "| epoch  43 |    45/   63 batches | ms/batch 109.08 | CE 0.1716 |\n",
      "| epoch  43 |    60/   63 batches | ms/batch 110.68 | CE 0.1699 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  43 | time:  6.92s | CE 0.1705 |\n",
      "    | end of validation epoch  43 | time:  0.85s | Accuracy 0.6267 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  44 |    15/   63 batches | ms/batch 121.07 | CE 0.1632 |\n",
      "| epoch  44 |    30/   63 batches | ms/batch 121.30 | CE 0.1602 |\n",
      "| epoch  44 |    45/   63 batches | ms/batch 122.38 | CE 0.1569 |\n",
      "| epoch  44 |    60/   63 batches | ms/batch 119.61 | CE 0.1561 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  44 | time:  7.52s | CE 0.1560 |\n",
      "    | end of validation epoch  44 | time:  0.89s | Accuracy 0.6333 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  45 |    15/   63 batches | ms/batch 121.80 | CE 0.1508 |\n",
      "| epoch  45 |    30/   63 batches | ms/batch 119.67 | CE 0.1468 |\n",
      "| epoch  45 |    45/   63 batches | ms/batch 119.56 | CE 0.1460 |\n",
      "| epoch  45 |    60/   63 batches | ms/batch 119.09 | CE 0.1429 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  45 | time:  7.44s | CE 0.1447 |\n",
      "    | end of validation epoch  45 | time:  0.86s | Accuracy 0.6267 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  46 |    15/   63 batches | ms/batch 119.13 | CE 0.1357 |\n",
      "| epoch  46 |    30/   63 batches | ms/batch 121.89 | CE 0.1333 |\n",
      "| epoch  46 |    45/   63 batches | ms/batch 124.06 | CE 0.1346 |\n",
      "| epoch  46 |    60/   63 batches | ms/batch 122.70 | CE 0.1336 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  46 | time:  7.64s | CE 0.1335 |\n",
      "    | end of validation epoch  46 | time:  0.80s | Accuracy 0.6267 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  47 |    15/   63 batches | ms/batch 113.85 | CE 0.1236 |\n",
      "| epoch  47 |    30/   63 batches | ms/batch 113.26 | CE 0.1239 |\n",
      "| epoch  47 |    45/   63 batches | ms/batch 111.42 | CE 0.1228 |\n",
      "| epoch  47 |    60/   63 batches | ms/batch 113.98 | CE 0.1230 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  47 | time:  7.14s | CE 0.1232 |\n",
      "    | end of validation epoch  47 | time:  0.79s | Accuracy 0.6267 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  48 |    15/   63 batches | ms/batch 103.73 | CE 0.1184 |\n",
      "| epoch  48 |    30/   63 batches | ms/batch 119.33 | CE 0.1131 |\n",
      "| epoch  48 |    45/   63 batches | ms/batch 119.34 | CE 0.1147 |\n",
      "| epoch  48 |    60/   63 batches | ms/batch 120.79 | CE 0.1142 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  48 | time:  7.62s | CE 0.1142 |\n",
      "    | end of validation epoch  48 | time:  0.88s | Accuracy 0.6400 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  49 |    15/   63 batches | ms/batch 133.11 | CE 0.1086 |\n",
      "| epoch  49 |    30/   63 batches | ms/batch 128.56 | CE 0.1087 |\n",
      "| epoch  49 |    45/   63 batches | ms/batch 125.04 | CE 0.1065 |\n",
      "| epoch  49 |    60/   63 batches | ms/batch 123.95 | CE 0.1067 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  49 | time:  7.74s | CE 0.1060 |\n",
      "    | end of validation epoch  49 | time:  0.87s | Accuracy 0.6333 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  50 |    15/   63 batches | ms/batch 112.10 | CE 0.1038 |\n",
      "| epoch  50 |    30/   63 batches | ms/batch 112.78 | CE 0.1005 |\n",
      "| epoch  50 |    45/   63 batches | ms/batch 117.70 | CE 0.0994 |\n",
      "| epoch  50 |    60/   63 batches | ms/batch 117.36 | CE 0.0992 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  50 | time:  7.36s | CE 0.0987 |\n",
      "    | end of validation epoch  50 | time:  0.82s | Accuracy 0.6267 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  51 |    15/   63 batches | ms/batch 114.58 | CE 0.0970 |\n",
      "| epoch  51 |    30/   63 batches | ms/batch 122.36 | CE 0.0949 |\n",
      "| epoch  51 |    45/   63 batches | ms/batch 121.01 | CE 0.0935 |\n",
      "| epoch  51 |    60/   63 batches | ms/batch 117.88 | CE 0.0927 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  51 | time:  7.35s | CE 0.0918 |\n",
      "    | end of validation epoch  51 | time:  0.79s | Accuracy 0.6133 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  52 |    15/   63 batches | ms/batch 115.01 | CE 0.0905 |\n",
      "| epoch  52 |    30/   63 batches | ms/batch 116.34 | CE 0.0884 |\n",
      "| epoch  52 |    45/   63 batches | ms/batch 117.93 | CE 0.0872 |\n",
      "| epoch  52 |    60/   63 batches | ms/batch 116.11 | CE 0.0859 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  52 | time:  7.27s | CE 0.0860 |\n",
      "    | end of validation epoch  52 | time:  0.88s | Accuracy 0.6200 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  53 |    15/   63 batches | ms/batch 112.35 | CE 0.0837 |\n",
      "| epoch  53 |    30/   63 batches | ms/batch 115.82 | CE 0.0824 |\n",
      "| epoch  53 |    45/   63 batches | ms/batch 117.13 | CE 0.0803 |\n",
      "| epoch  53 |    60/   63 batches | ms/batch 119.89 | CE 0.0807 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  53 | time:  7.46s | CE 0.0803 |\n",
      "    | end of validation epoch  53 | time:  0.92s | Accuracy 0.6067 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  54 |    15/   63 batches | ms/batch 138.89 | CE 0.0743 |\n",
      "| epoch  54 |    30/   63 batches | ms/batch 130.83 | CE 0.0750 |\n",
      "| epoch  54 |    45/   63 batches | ms/batch 125.32 | CE 0.0755 |\n",
      "| epoch  54 |    60/   63 batches | ms/batch 123.40 | CE 0.0755 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  54 | time:  7.67s | CE 0.0753 |\n",
      "    | end of validation epoch  54 | time:  0.79s | Accuracy 0.6133 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  55 |    15/   63 batches | ms/batch 109.14 | CE 0.0734 |\n",
      "| epoch  55 |    30/   63 batches | ms/batch 106.70 | CE 0.0726 |\n",
      "| epoch  55 |    45/   63 batches | ms/batch 105.48 | CE 0.0713 |\n",
      "| epoch  55 |    60/   63 batches | ms/batch 108.27 | CE 0.0708 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  55 | time:  6.82s | CE 0.0707 |\n",
      "    | end of validation epoch  55 | time:  0.82s | Accuracy 0.6200 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  56 |    15/   63 batches | ms/batch 116.26 | CE 0.0682 |\n",
      "| epoch  56 |    30/   63 batches | ms/batch 118.55 | CE 0.0675 |\n",
      "| epoch  56 |    45/   63 batches | ms/batch 115.53 | CE 0.0674 |\n",
      "| epoch  56 |    60/   63 batches | ms/batch 115.71 | CE 0.0665 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  56 | time:  7.20s | CE 0.0665 |\n",
      "    | end of validation epoch  56 | time:  0.89s | Accuracy 0.6133 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  57 |    15/   63 batches | ms/batch 109.49 | CE 0.0641 |\n",
      "| epoch  57 |    30/   63 batches | ms/batch 112.39 | CE 0.0618 |\n",
      "| epoch  57 |    45/   63 batches | ms/batch 111.73 | CE 0.0622 |\n",
      "| epoch  57 |    60/   63 batches | ms/batch 113.48 | CE 0.0626 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  57 | time:  7.14s | CE 0.0623 |\n",
      "    | end of validation epoch  57 | time:  0.78s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  58 |    15/   63 batches | ms/batch 111.82 | CE 0.0589 |\n",
      "| epoch  58 |    30/   63 batches | ms/batch 110.60 | CE 0.0603 |\n",
      "| epoch  58 |    45/   63 batches | ms/batch 108.90 | CE 0.0594 |\n",
      "| epoch  58 |    60/   63 batches | ms/batch 107.43 | CE 0.0589 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  58 | time:  6.71s | CE 0.0589 |\n",
      "    | end of validation epoch  58 | time:  0.93s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  59 |    15/   63 batches | ms/batch 125.74 | CE 0.0565 |\n",
      "| epoch  59 |    30/   63 batches | ms/batch 124.07 | CE 0.0570 |\n",
      "| epoch  59 |    45/   63 batches | ms/batch 120.55 | CE 0.0564 |\n",
      "| epoch  59 |    60/   63 batches | ms/batch 119.92 | CE 0.0557 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  59 | time:  7.54s | CE 0.0556 |\n",
      "    | end of validation epoch  59 | time:  0.99s | Accuracy 0.6133 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  60 |    15/   63 batches | ms/batch 123.98 | CE 0.0526 |\n",
      "| epoch  60 |    30/   63 batches | ms/batch 131.64 | CE 0.0533 |\n",
      "| epoch  60 |    45/   63 batches | ms/batch 128.04 | CE 0.0532 |\n",
      "| epoch  60 |    60/   63 batches | ms/batch 127.26 | CE 0.0525 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  60 | time:  7.92s | CE 0.0525 |\n",
      "    | end of validation epoch  60 | time:  0.86s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  61 |    15/   63 batches | ms/batch 111.49 | CE 0.0501 |\n",
      "| epoch  61 |    30/   63 batches | ms/batch 115.77 | CE 0.0510 |\n",
      "| epoch  61 |    45/   63 batches | ms/batch 117.79 | CE 0.0501 |\n",
      "| epoch  61 |    60/   63 batches | ms/batch 118.24 | CE 0.0499 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  61 | time:  7.40s | CE 0.0496 |\n",
      "    | end of validation epoch  61 | time:  0.84s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  62 |    15/   63 batches | ms/batch 112.26 | CE 0.0476 |\n",
      "| epoch  62 |    30/   63 batches | ms/batch 106.77 | CE 0.0466 |\n",
      "| epoch  62 |    45/   63 batches | ms/batch 104.47 | CE 0.0469 |\n",
      "| epoch  62 |    60/   63 batches | ms/batch 110.62 | CE 0.0469 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  62 | time:  6.96s | CE 0.0471 |\n",
      "    | end of validation epoch  62 | time:  0.78s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  63 |    15/   63 batches | ms/batch 108.09 | CE 0.0455 |\n",
      "| epoch  63 |    30/   63 batches | ms/batch 105.34 | CE 0.0451 |\n",
      "| epoch  63 |    45/   63 batches | ms/batch 104.83 | CE 0.0448 |\n",
      "| epoch  63 |    60/   63 batches | ms/batch 104.34 | CE 0.0446 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  63 | time:  6.52s | CE 0.0446 |\n",
      "    | end of validation epoch  63 | time:  0.79s | Accuracy 0.6067 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  64 |    15/   63 batches | ms/batch 99.59 | CE 0.0439 |\n",
      "| epoch  64 |    30/   63 batches | ms/batch 99.54 | CE 0.0428 |\n",
      "| epoch  64 |    45/   63 batches | ms/batch 99.73 | CE 0.0432 |\n",
      "| epoch  64 |    60/   63 batches | ms/batch 99.45 | CE 0.0425 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  64 | time:  6.22s | CE 0.0424 |\n",
      "    | end of validation epoch  64 | time:  0.78s | Accuracy 0.6133 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  65 |    15/   63 batches | ms/batch 99.66 | CE 0.0406 |\n",
      "| epoch  65 |    30/   63 batches | ms/batch 100.09 | CE 0.0403 |\n",
      "| epoch  65 |    45/   63 batches | ms/batch 102.94 | CE 0.0401 |\n",
      "| epoch  65 |    60/   63 batches | ms/batch 113.04 | CE 0.0404 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  65 | time:  7.10s | CE 0.0403 |\n",
      "    | end of validation epoch  65 | time:  0.99s | Accuracy 0.6067 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  66 |    15/   63 batches | ms/batch 172.02 | CE 0.0388 |\n",
      "| epoch  66 |    30/   63 batches | ms/batch 174.52 | CE 0.0383 |\n",
      "| epoch  66 |    45/   63 batches | ms/batch 170.83 | CE 0.0384 |\n",
      "| epoch  66 |    60/   63 batches | ms/batch 165.18 | CE 0.0383 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  66 | time: 10.32s | CE 0.0383 |\n",
      "    | end of validation epoch  66 | time:  1.16s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  67 |    15/   63 batches | ms/batch 149.00 | CE 0.0363 |\n",
      "| epoch  67 |    30/   63 batches | ms/batch 133.55 | CE 0.0367 |\n",
      "| epoch  67 |    45/   63 batches | ms/batch 125.80 | CE 0.0367 |\n",
      "| epoch  67 |    60/   63 batches | ms/batch 123.04 | CE 0.0364 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  67 | time:  7.69s | CE 0.0364 |\n",
      "    | end of validation epoch  67 | time:  0.83s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  68 |    15/   63 batches | ms/batch 116.68 | CE 0.0351 |\n",
      "| epoch  68 |    30/   63 batches | ms/batch 119.80 | CE 0.0344 |\n",
      "| epoch  68 |    45/   63 batches | ms/batch 118.49 | CE 0.0346 |\n",
      "| epoch  68 |    60/   63 batches | ms/batch 119.18 | CE 0.0347 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  68 | time:  7.43s | CE 0.0347 |\n",
      "    | end of validation epoch  68 | time:  0.79s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  69 |    15/   63 batches | ms/batch 107.47 | CE 0.0330 |\n",
      "| epoch  69 |    30/   63 batches | ms/batch 109.37 | CE 0.0332 |\n",
      "| epoch  69 |    45/   63 batches | ms/batch 113.04 | CE 0.0331 |\n",
      "| epoch  69 |    60/   63 batches | ms/batch 113.67 | CE 0.0331 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  69 | time:  7.11s | CE 0.0330 |\n",
      "    | end of validation epoch  69 | time:  0.80s | Accuracy 0.5733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  70 |    15/   63 batches | ms/batch 141.23 | CE 0.0319 |\n",
      "| epoch  70 |    30/   63 batches | ms/batch 129.37 | CE 0.0319 |\n",
      "| epoch  70 |    45/   63 batches | ms/batch 125.99 | CE 0.0317 |\n",
      "| epoch  70 |    60/   63 batches | ms/batch 124.23 | CE 0.0316 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  70 | time:  7.74s | CE 0.0315 |\n",
      "    | end of validation epoch  70 | time:  0.81s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  71 |    15/   63 batches | ms/batch 131.69 | CE 0.0301 |\n",
      "| epoch  71 |    30/   63 batches | ms/batch 125.08 | CE 0.0298 |\n",
      "| epoch  71 |    45/   63 batches | ms/batch 129.64 | CE 0.0301 |\n",
      "| epoch  71 |    60/   63 batches | ms/batch 124.98 | CE 0.0300 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  71 | time:  7.77s | CE 0.0300 |\n",
      "    | end of validation epoch  71 | time:  0.79s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  72 |    15/   63 batches | ms/batch 112.25 | CE 0.0289 |\n",
      "| epoch  72 |    30/   63 batches | ms/batch 112.83 | CE 0.0285 |\n",
      "| epoch  72 |    45/   63 batches | ms/batch 113.90 | CE 0.0287 |\n",
      "| epoch  72 |    60/   63 batches | ms/batch 116.27 | CE 0.0286 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  72 | time:  7.27s | CE 0.0285 |\n",
      "    | end of validation epoch  72 | time:  0.80s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  73 |    15/   63 batches | ms/batch 112.71 | CE 0.0266 |\n",
      "| epoch  73 |    30/   63 batches | ms/batch 114.00 | CE 0.0270 |\n",
      "| epoch  73 |    45/   63 batches | ms/batch 113.32 | CE 0.0272 |\n",
      "| epoch  73 |    60/   63 batches | ms/batch 113.71 | CE 0.0273 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  73 | time:  7.10s | CE 0.0272 |\n",
      "    | end of validation epoch  73 | time:  0.86s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  74 |    15/   63 batches | ms/batch 117.21 | CE 0.0261 |\n",
      "| epoch  74 |    30/   63 batches | ms/batch 117.66 | CE 0.0267 |\n",
      "| epoch  74 |    45/   63 batches | ms/batch 119.82 | CE 0.0262 |\n",
      "| epoch  74 |    60/   63 batches | ms/batch 118.54 | CE 0.0260 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  74 | time:  7.40s | CE 0.0260 |\n",
      "    | end of validation epoch  74 | time:  0.81s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  75 |    15/   63 batches | ms/batch 112.50 | CE 0.0250 |\n",
      "| epoch  75 |    30/   63 batches | ms/batch 115.25 | CE 0.0250 |\n",
      "| epoch  75 |    45/   63 batches | ms/batch 114.20 | CE 0.0250 |\n",
      "| epoch  75 |    60/   63 batches | ms/batch 114.99 | CE 0.0249 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  75 | time:  7.16s | CE 0.0248 |\n",
      "    | end of validation epoch  75 | time:  1.09s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  76 |    15/   63 batches | ms/batch 126.28 | CE 0.0247 |\n",
      "| epoch  76 |    30/   63 batches | ms/batch 120.43 | CE 0.0243 |\n",
      "| epoch  76 |    45/   63 batches | ms/batch 120.04 | CE 0.0240 |\n",
      "| epoch  76 |    60/   63 batches | ms/batch 118.52 | CE 0.0239 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  76 | time:  7.41s | CE 0.0238 |\n",
      "    | end of validation epoch  76 | time:  0.83s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  77 |    15/   63 batches | ms/batch 108.17 | CE 0.0236 |\n",
      "| epoch  77 |    30/   63 batches | ms/batch 108.20 | CE 0.0233 |\n",
      "| epoch  77 |    45/   63 batches | ms/batch 110.21 | CE 0.0230 |\n",
      "| epoch  77 |    60/   63 batches | ms/batch 112.16 | CE 0.0228 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  77 | time:  7.03s | CE 0.0228 |\n",
      "    | end of validation epoch  77 | time:  0.88s | Accuracy 0.6000 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  78 |    15/   63 batches | ms/batch 114.52 | CE 0.0217 |\n",
      "| epoch  78 |    30/   63 batches | ms/batch 131.69 | CE 0.0218 |\n",
      "| epoch  78 |    45/   63 batches | ms/batch 130.36 | CE 0.0219 |\n",
      "| epoch  78 |    60/   63 batches | ms/batch 129.35 | CE 0.0218 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  78 | time:  8.08s | CE 0.0218 |\n",
      "    | end of validation epoch  78 | time:  0.98s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  79 |    15/   63 batches | ms/batch 129.79 | CE 0.0211 |\n",
      "| epoch  79 |    30/   63 batches | ms/batch 125.88 | CE 0.0211 |\n",
      "| epoch  79 |    45/   63 batches | ms/batch 123.22 | CE 0.0211 |\n",
      "| epoch  79 |    60/   63 batches | ms/batch 121.05 | CE 0.0209 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  79 | time:  7.55s | CE 0.0208 |\n",
      "    | end of validation epoch  79 | time:  0.80s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  80 |    15/   63 batches | ms/batch 115.85 | CE 0.0199 |\n",
      "| epoch  80 |    30/   63 batches | ms/batch 113.97 | CE 0.0199 |\n",
      "| epoch  80 |    45/   63 batches | ms/batch 111.23 | CE 0.0200 |\n",
      "| epoch  80 |    60/   63 batches | ms/batch 112.14 | CE 0.0199 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  80 | time:  7.05s | CE 0.0200 |\n",
      "    | end of validation epoch  80 | time:  0.82s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  81 |    15/   63 batches | ms/batch 114.64 | CE 0.0191 |\n",
      "| epoch  81 |    30/   63 batches | ms/batch 120.54 | CE 0.0190 |\n",
      "| epoch  81 |    45/   63 batches | ms/batch 126.87 | CE 0.0191 |\n",
      "| epoch  81 |    60/   63 batches | ms/batch 124.35 | CE 0.0191 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  81 | time:  7.76s | CE 0.0191 |\n",
      "    | end of validation epoch  81 | time:  0.94s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  82 |    15/   63 batches | ms/batch 117.78 | CE 0.0185 |\n",
      "| epoch  82 |    30/   63 batches | ms/batch 114.28 | CE 0.0184 |\n",
      "| epoch  82 |    45/   63 batches | ms/batch 110.60 | CE 0.0183 |\n",
      "| epoch  82 |    60/   63 batches | ms/batch 110.01 | CE 0.0183 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  82 | time:  6.89s | CE 0.0184 |\n",
      "    | end of validation epoch  82 | time:  0.81s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  83 |    15/   63 batches | ms/batch 120.12 | CE 0.0186 |\n",
      "| epoch  83 |    30/   63 batches | ms/batch 115.10 | CE 0.0181 |\n",
      "| epoch  83 |    45/   63 batches | ms/batch 112.86 | CE 0.0179 |\n",
      "| epoch  83 |    60/   63 batches | ms/batch 112.05 | CE 0.0178 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  83 | time:  7.03s | CE 0.0177 |\n",
      "    | end of validation epoch  83 | time:  0.80s | Accuracy 0.5733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  84 |    15/   63 batches | ms/batch 135.77 | CE 0.0170 |\n",
      "| epoch  84 |    30/   63 batches | ms/batch 123.85 | CE 0.0169 |\n",
      "| epoch  84 |    45/   63 batches | ms/batch 119.54 | CE 0.0169 |\n",
      "| epoch  84 |    60/   63 batches | ms/batch 116.92 | CE 0.0170 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  84 | time:  7.30s | CE 0.0170 |\n",
      "    | end of validation epoch  84 | time:  0.78s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  85 |    15/   63 batches | ms/batch 110.90 | CE 0.0161 |\n",
      "| epoch  85 |    30/   63 batches | ms/batch 120.20 | CE 0.0164 |\n",
      "| epoch  85 |    45/   63 batches | ms/batch 122.43 | CE 0.0164 |\n",
      "| epoch  85 |    60/   63 batches | ms/batch 119.57 | CE 0.0164 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  85 | time:  7.58s | CE 0.0164 |\n",
      "    | end of validation epoch  85 | time:  0.99s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  86 |    15/   63 batches | ms/batch 113.73 | CE 0.0162 |\n",
      "| epoch  86 |    30/   63 batches | ms/batch 112.76 | CE 0.0161 |\n",
      "| epoch  86 |    45/   63 batches | ms/batch 115.36 | CE 0.0159 |\n",
      "| epoch  86 |    60/   63 batches | ms/batch 116.35 | CE 0.0157 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  86 | time:  7.30s | CE 0.0157 |\n",
      "    | end of validation epoch  86 | time:  0.84s | Accuracy 0.5933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  87 |    15/   63 batches | ms/batch 108.57 | CE 0.0155 |\n",
      "| epoch  87 |    30/   63 batches | ms/batch 107.14 | CE 0.0153 |\n",
      "| epoch  87 |    45/   63 batches | ms/batch 111.06 | CE 0.0151 |\n",
      "| epoch  87 |    60/   63 batches | ms/batch 112.96 | CE 0.0150 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  87 | time:  7.07s | CE 0.0150 |\n",
      "    | end of validation epoch  87 | time:  0.81s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  88 |    15/   63 batches | ms/batch 119.33 | CE 0.0146 |\n",
      "| epoch  88 |    30/   63 batches | ms/batch 122.70 | CE 0.0144 |\n",
      "| epoch  88 |    45/   63 batches | ms/batch 120.48 | CE 0.0145 |\n",
      "| epoch  88 |    60/   63 batches | ms/batch 120.09 | CE 0.0144 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  88 | time:  7.49s | CE 0.0144 |\n",
      "    | end of validation epoch  88 | time:  0.88s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  89 |    15/   63 batches | ms/batch 116.94 | CE 0.0138 |\n",
      "| epoch  89 |    30/   63 batches | ms/batch 118.34 | CE 0.0140 |\n",
      "| epoch  89 |    45/   63 batches | ms/batch 121.39 | CE 0.0138 |\n",
      "| epoch  89 |    60/   63 batches | ms/batch 119.11 | CE 0.0138 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  89 | time:  7.43s | CE 0.0138 |\n",
      "    | end of validation epoch  89 | time:  0.92s | Accuracy 0.5733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  90 |    15/   63 batches | ms/batch 129.64 | CE 0.0134 |\n",
      "| epoch  90 |    30/   63 batches | ms/batch 127.77 | CE 0.0135 |\n",
      "| epoch  90 |    45/   63 batches | ms/batch 132.26 | CE 0.0133 |\n",
      "| epoch  90 |    60/   63 batches | ms/batch 130.23 | CE 0.0133 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  90 | time:  8.08s | CE 0.0133 |\n",
      "    | end of validation epoch  90 | time:  0.81s | Accuracy 0.5733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  91 |    15/   63 batches | ms/batch 114.45 | CE 0.0129 |\n",
      "| epoch  91 |    30/   63 batches | ms/batch 113.01 | CE 0.0127 |\n",
      "| epoch  91 |    45/   63 batches | ms/batch 118.64 | CE 0.0128 |\n",
      "| epoch  91 |    60/   63 batches | ms/batch 117.67 | CE 0.0128 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  91 | time:  7.33s | CE 0.0128 |\n",
      "    | end of validation epoch  91 | time:  0.77s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  92 |    15/   63 batches | ms/batch 128.11 | CE 0.0125 |\n",
      "| epoch  92 |    30/   63 batches | ms/batch 128.28 | CE 0.0125 |\n",
      "| epoch  92 |    45/   63 batches | ms/batch 127.75 | CE 0.0124 |\n",
      "| epoch  92 |    60/   63 batches | ms/batch 125.90 | CE 0.0123 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  92 | time:  7.89s | CE 0.0123 |\n",
      "    | end of validation epoch  92 | time:  0.82s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  93 |    15/   63 batches | ms/batch 123.00 | CE 0.0121 |\n",
      "| epoch  93 |    30/   63 batches | ms/batch 121.63 | CE 0.0119 |\n",
      "| epoch  93 |    45/   63 batches | ms/batch 122.37 | CE 0.0119 |\n",
      "| epoch  93 |    60/   63 batches | ms/batch 118.04 | CE 0.0118 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  93 | time:  7.35s | CE 0.0118 |\n",
      "    | end of validation epoch  93 | time:  0.96s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  94 |    15/   63 batches | ms/batch 122.16 | CE 0.0114 |\n",
      "| epoch  94 |    30/   63 batches | ms/batch 118.46 | CE 0.0114 |\n",
      "| epoch  94 |    45/   63 batches | ms/batch 116.07 | CE 0.0114 |\n",
      "| epoch  94 |    60/   63 batches | ms/batch 115.48 | CE 0.0113 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  94 | time:  7.20s | CE 0.0113 |\n",
      "    | end of validation epoch  94 | time:  0.80s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  95 |    15/   63 batches | ms/batch 121.78 | CE 0.0112 |\n",
      "| epoch  95 |    30/   63 batches | ms/batch 115.05 | CE 0.0110 |\n",
      "| epoch  95 |    45/   63 batches | ms/batch 111.03 | CE 0.0111 |\n",
      "| epoch  95 |    60/   63 batches | ms/batch 109.64 | CE 0.0109 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  95 | time:  6.83s | CE 0.0109 |\n",
      "    | end of validation epoch  95 | time:  0.77s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  96 |    15/   63 batches | ms/batch 125.79 | CE 0.0110 |\n",
      "| epoch  96 |    30/   63 batches | ms/batch 118.53 | CE 0.0107 |\n",
      "| epoch  96 |    45/   63 batches | ms/batch 118.08 | CE 0.0107 |\n",
      "| epoch  96 |    60/   63 batches | ms/batch 117.91 | CE 0.0105 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  96 | time:  7.40s | CE 0.0105 |\n",
      "    | end of validation epoch  96 | time:  0.84s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  97 |    15/   63 batches | ms/batch 134.24 | CE 0.0103 |\n",
      "| epoch  97 |    30/   63 batches | ms/batch 135.94 | CE 0.0102 |\n",
      "| epoch  97 |    45/   63 batches | ms/batch 130.04 | CE 0.0102 |\n",
      "| epoch  97 |    60/   63 batches | ms/batch 128.22 | CE 0.0101 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  97 | time:  8.05s | CE 0.0101 |\n",
      "    | end of validation epoch  97 | time:  0.91s | Accuracy 0.5867 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  98 |    15/   63 batches | ms/batch 131.14 | CE 0.0099 |\n",
      "| epoch  98 |    30/   63 batches | ms/batch 124.48 | CE 0.0099 |\n",
      "| epoch  98 |    45/   63 batches | ms/batch 120.13 | CE 0.0099 |\n",
      "| epoch  98 |    60/   63 batches | ms/batch 118.92 | CE 0.0098 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  98 | time:  7.40s | CE 0.0098 |\n",
      "    | end of validation epoch  98 | time:  0.87s | Accuracy 0.5733 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  99 |    15/   63 batches | ms/batch 142.32 | CE 0.0096 |\n",
      "| epoch  99 |    30/   63 batches | ms/batch 130.65 | CE 0.0095 |\n",
      "| epoch  99 |    45/   63 batches | ms/batch 132.68 | CE 0.0095 |\n",
      "| epoch  99 |    60/   63 batches | ms/batch 132.11 | CE 0.0094 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  99 | time:  8.28s | CE 0.0094 |\n",
      "    | end of validation epoch  99 | time:  0.91s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch 100 |    15/   63 batches | ms/batch 128.65 | CE 0.0091 |\n",
      "| epoch 100 |    30/   63 batches | ms/batch 128.80 | CE 0.0091 |\n",
      "| epoch 100 |    45/   63 batches | ms/batch 131.83 | CE 0.0091 |\n",
      "| epoch 100 |    60/   63 batches | ms/batch 131.39 | CE 0.0091 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch 100 | time:  8.15s | CE 0.0091 |\n",
      "    | end of validation epoch 100 | time:  0.82s | Accuracy 0.5800 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: train the model on the dataset above\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "class dataset_pipeline(Dataset):\n",
    "    def __init__(self, data, label):\n",
    "        super(dataset_pipeline, self).__init__()\n",
    "        \n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "        self._len = len(self.data)  # number of utterances\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # calculate STFT here\n",
    "        spec = librosa.stft(self.data[index].astype(np.float32), n_fft=512, hop_length=256)\n",
    "        label = self.label[index] - 1\n",
    "        spec = torch.from_numpy(np.abs(spec))  # only use the magnitude spectrogram\n",
    "        label = torch.from_numpy(np.array(label)).long()\n",
    "            \n",
    "        return spec, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "# define data loaders\n",
    "train_loader = DataLoader(dataset_pipeline(train_audio, train_target), \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True,\n",
    "                         )\n",
    "\n",
    "validation_loader = DataLoader(dataset_pipeline(val_audio, val_target), \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=False,\n",
    "                              )\n",
    "\n",
    "dataset_len = len(train_loader)\n",
    "log_step = dataset_len // 4\n",
    "\n",
    "# CE loss\n",
    "def CE(output, target):\n",
    "    # output shape: (batch, num_classes)\n",
    "    # target shape: (batch,)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return loss(output, target)\n",
    "\n",
    "\n",
    "def train(model, epoch, versatile=True):\n",
    "    start_time = time.time()\n",
    "    model = model.train()  # set the model to training mode. Always do this before you start training!\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        spec, label = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, output_embedding = model(spec.unsqueeze(1))\n",
    "        \n",
    "        # CE as objective\n",
    "        loss = CE(output, label)\n",
    "        \n",
    "        # automatically calculate the backward pass\n",
    "        loss.backward()\n",
    "        # perform the actual backpropagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data.item()\n",
    "        \n",
    "        # OPTIONAL: you can print the training progress \n",
    "        if versatile:\n",
    "            if (batch_idx+1) % log_step == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | CE {:5.4f} |'.format(\n",
    "                    epoch, batch_idx+1, len(train_loader),\n",
    "                    elapsed * 1000 / (batch_idx+1), \n",
    "                    train_loss / (batch_idx+1)\n",
    "                    ))\n",
    "    \n",
    "    train_loss /= (batch_idx+1)\n",
    "    print('-' * 99)\n",
    "    print('    | end of training epoch {:3d} | time: {:5.2f}s | CE {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "        \n",
    "def validate(model, epoch):\n",
    "    start_time = time.time()\n",
    "    model = model.eval()  # set the model to evaluation mode. Always do this during validation or test phase!\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        spec, label = data\n",
    "        \n",
    "        # you don't need to calculate the backward pass and the gradients during validation\n",
    "        # so you can call torch.no_grad() to only calculate the forward pass to save time and memory\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            output, output_embedding = model(spec.unsqueeze(1))\n",
    "        \n",
    "            # calculate accuracy\n",
    "            _, output_label = torch.max(output, 1)\n",
    "            output_label = output_label.data.numpy()\n",
    "            label = label.data.numpy()\n",
    "            correct += np.sum(output_label == label)\n",
    "            total += len(label)\n",
    "        \n",
    "    accuracy = correct / total\n",
    "    print('    | end of validation epoch {:3d} | time: {:5.2f}s | Accuracy {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), accuracy))\n",
    "    print('-' * 99)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "total_epoch = 100  # train the model for 100 epochs\n",
    "model_save = 'best_AlexNet_SV.pt'  # path to save the best validation model\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# main function\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    training_loss.append(train(model, epoch))\n",
    "    validation_loss.append(validate(model, epoch))\n",
    "    \n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "    if validation_loss[-1] == np.max(validation_loss):\n",
    "        # save current best model on validation set\n",
    "        with open(model_save, 'wb') as f:\n",
    "            torch.save(model.state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "    \n",
    "    print('-' * 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 60.00%\n"
     ]
    }
   ],
   "source": [
    "# TODO: evaluate your best model on the test set\n",
    "\n",
    "# load the best model on validation set\n",
    "model.load_state_dict(torch.load('best_AlexNet_SV.pt'))\n",
    "model.eval()\n",
    "\n",
    "correct = []\n",
    "total = len(test_audio)\n",
    "\n",
    "for i in range(len(test_audio)):\n",
    "    this_spec = librosa.stft(test_audio[i].astype(np.float32), n_fft=512, hop_length=256)\n",
    "    this_label = test_target[i] - 1\n",
    "    spec = torch.from_numpy(np.abs(this_spec))  # only use the magnitude spectrogram\n",
    "    this_label = torch.from_numpy(np.array(this_label)).long()\n",
    "    \n",
    "    output, output_embedding = model(spec.unsqueeze(0).unsqueeze(1))\n",
    "        \n",
    "    # calculate accuracy\n",
    "    _, output_label = torch.max(output, 1)\n",
    "    output_label = output_label.data.numpy()\n",
    "    this_label = this_label.data.numpy()\n",
    "    correct.append(np.sum(output_label == this_label))\n",
    "\n",
    "print('Overall accuracy: {:.2f}%'.format(np.sum(correct) / total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the acoustic event detection task, the estimated target class can be directly used as what we want is indeed the class label. However, in speaker recognition task we have another step - **scoring**. This stage is useful here because in many cases the query speaker might not match any of the saved speakers, and we might not be able to assign a proper speaker label to it. In speaker identification task, this means that we cannot always perform *argmax* on the network output to assign the corresponding speaker label to it. In speaker verification task, this means that we can only say the query speaker matches the target speaker when the similarity score is higher than a (predefined) threshold. Acoustic event detection task often assumes that all the possible types of audio events are known and there won't be out-of-box event types, and there is also no need to perform such verification process for such scenes.\n",
    "\n",
    "One of the most simple way to perform scoring is to compare the speaker embedding from the query utterance to the overall characteristics from the training set. The overall characteristics for a target speaker is generated by averaging across all the speaker embeddings of the speaker's training utterances. You can intuitively treat it as a clustering process, and the average embedding is essentially the cluster center for all the possible speaker embeddings for one speaker.\n",
    "\n",
    "Extract all the training speaker embeddings below and calculate the average speaker embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: extract speaker embeddings for each of the speaker's training utterance\n",
    "# use the best model on validation set above\n",
    "# save all training speaker embeddings as all_embedding (shape: (500, 256) for (num_spk*num_utterance, embedding_dim))\n",
    "# save average speaker embeddings as average_embedding (shape: (50, 256) for (num_spk, embedding_dim))\n",
    "# remember to convert them to numpy.array format!\n",
    "\n",
    "all_embedding = []\n",
    "\n",
    "for i in range(len(train_audio)):\n",
    "    this_spec = librosa.stft(train_audio[i].astype(np.float32), n_fft=512, hop_length=256)\n",
    "    spec = torch.from_numpy(np.abs(this_spec))  # only use the magnitude spectrogram\n",
    "    \n",
    "    output, output_embedding = model(spec.unsqueeze(0).unsqueeze(1))\n",
    "    all_embedding.append(output_embedding)\n",
    "    \n",
    "all_embedding = torch.cat(all_embedding, 0).data.numpy()  # 500, 256\n",
    "average_embedding = np.mean(all_embedding.reshape(50, 10, -1), 1)  # 50, 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the embeddings. Note that the embeddings are pretty high-dimensional (256-dimensional) and it is impossible to directly visualize them. We need certain **dimension reduction** methods to map them to a low-dimensional space (2 or 3-dimensional space) for us to learn about their properties. The most simply way to perform dimension reduction is **principal component analysis (PCA)**, and let's use the [*sklearn.decomposition.PCA*](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to do it.\n",
    "\n",
    "Apply the PCA function to the ***all_embedding*** matrix, set the number of reduced dimension to 2 (*n_components=2*), and visualize the embeddings for the first 5 speakers (first 50 embeddings in *all_embedding*) by plotting a scatter plot. Use different colors for different speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: perform PCA on the embeddings, visualize the embeddings for the first 5 speakers\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "all_embedding_PCA = pca.fit_transform(all_embedding)\n",
    "\n",
    "cmaps = ['r', 'g', 'b', 'y', 'm']\n",
    "\n",
    "# plot the first 70 embeddings\n",
    "for i in range(10):\n",
    "    for j in range(5):\n",
    "        plt.scatter(all_embedding_PCA[i+j*10][0], all_embedding_PCA[i+j*10][1], c=cmaps[j])\n",
    "plt.legend(['spk'+str(i+1) for i in range(5)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that embeddings for different speakers are separate apart and each speaker has a cluster. For a query speaker utterance, we can compute its score with respect to all the speakers in the training utterances by calculating a similarity measure between the speaker embedding for this query utterance and all the average speaker embeddings. One of such similarity measure is **cosine similarity**, and let's apply it on our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: calculate the cosine similarity scores between each utterance in the test set and all the average embeddings\n",
    "# save your cosine similarity scores in cos_sim (shape: (50, 50) for (num_target_spk, num_test_utterance))\n",
    "\n",
    "test_embedding = []\n",
    "\n",
    "for i in range(len(test_audio)):\n",
    "    this_spec = librosa.stft(test_audio[i].astype(np.float32), n_fft=512, hop_length=256)\n",
    "    spec = torch.from_numpy(np.abs(this_spec))  # only use the magnitude spectrogram\n",
    "    \n",
    "    output, output_embedding = model(spec.unsqueeze(0).unsqueeze(1))\n",
    "    test_embedding.append(output_embedding)\n",
    "    \n",
    "test_embedding = torch.cat(test_embedding, 0).data.numpy()  # 50, 256\n",
    "\n",
    "cos_sim = np.dot(test_embedding, average_embedding.T) / np.sqrt(np.sum(average_embedding**2, 1)[:,np.newaxis] + 1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a 50*50 matrix of all the cosine similarity scores. Let's visualize it as a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the cosine similarity scores as a confusion matrix\n",
    "\n",
    "# this is the most simple way to plot it\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cos_sim)\n",
    "plt.xlabel('Test utterance (speaker) index', fontsize=16)\n",
    "plt.ylabel('Train speaker index', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may observe that for a successfull speaker recognition, the elements on the diagonal typically present the highest score for each test utterance. This is indeed what we expect.\n",
    "\n",
    "Now we need a way to properly evaluate our system. We had the classification accuracy above which can be used for the identification task where we assume that we know all the target speakers to be identified in advance. For verification task which is indeed a binary classification task (accept/reject), we need other ways to determine the threshold for the similarity scores as well as measure the system's performance. One widely-used metric for evaluation in the verification task is the ***equal error rate (EER)***, defined as a value when [Type I = Type II error](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors). It can be derived from the [***receiver operating characteristic (ROC) curve***](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics). I will not go into the details here and you can refer to the links for further information as well as its usage. A sample implementation of EER is directly provided here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall EER: 12.00%; decision threshold for similarity scores: 0.25\n"
     ]
    }
   ],
   "source": [
    "# EER calculation with ROC curve\n",
    "# adopted from https://yangcha.github.io/EER-ROC/\n",
    "\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def EER(y, y_pred):\n",
    "    # y_pred is a list of similarity scores for the verification task (cosine similarity values)\n",
    "    # y is a list of the binary labels (accept/reject), where 1 is used for acceptance\n",
    "    fpr, tpr, threshold = roc_curve(y, y_pred, pos_label=1)\n",
    "    \n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    decision_threshold = interp1d(fpr, threshold)(eer)\n",
    "    \n",
    "    return eer, decision_threshold\n",
    "\n",
    "# example usage on the test utterances\n",
    "# verify if each test utterance matches each of the target speakers\n",
    "# a total of 50*50=2500 verification tasks\n",
    "\n",
    "label = np.diag(np.ones(50)).reshape(-1).astype(np.float32)\n",
    "eer, decision_threshold = EER(label, cos_sim.reshape(-1))\n",
    "print('Overall EER: {:.2f}%; decision threshold for similarity scores: {:.2f}'.format(eer*100, decision_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Siamese Network and Triplet Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have implemented above is an example pipeline - we train a standard multiclass classification system, use the feature as speaker embeddings, and identify (classify) or verify (score) them for the two tasks. However, there are other ways that we can train the model. Here we introduce another pipeline called ***siamese network and triplet loss***.\n",
    "\n",
    "Let's first take a look at this illustration (https://levelup.gitconnected.com/metric-learning-using-siamese-and-triplet-convolutional-neural-networks-ed5b01d83be3).\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*MIPdyhJGx6uLiob9UI9S0w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have three input images in this illustration. The term ***siamese network*** corresponds to the design that all three input images share the same network (same architecture and same parameters), and the term ***triplet loss*** corresponds to the training objective function applied to the three inputs. The three inputs, $x_a$, $x_p$ and $x_n$, represent the ***a*nchor input**, ***p*ositive sample**, and the ***n*egative sample**, respectively. In our case of speaker recognition, the anchor input is an utterance from a random speaker A, the positive sample is another utterance from the same speaker A, and the negative sample is an utterance from a random speaker B different from A. \n",
    "\n",
    "The triplet loss tries to **maximize** the similarity between the anchor input and the positive sample, and to **minimize** the similarity between the anchor input and the negative sample. The $\\alpha$ term is a predefined nonnegative scalar determining the minimum margin of the difference between the two similarity scores. In our case of speaker recognition where cosine similarity is used for the similarity score, it is equivalent to the following objective:\n",
    "$$\\theta_{a,p} > \\theta_{a,n} + \\alpha$$\n",
    "where $\\theta$ corresponds to the cosine similarity. The triplet loss can then be written as:\n",
    "$$L_{triplet} = max(\\theta_{a,n} - \\theta_{a,p} + \\alpha , 0)$$\n",
    "\n",
    "When $\\theta_{a,p} > \\theta_{a,n} + \\alpha$, $max(\\theta_{a,n} - \\theta_{a,p} + \\alpha , 0) = 0$ and there's no gradient with respect to the cosine similarity scores. This means that the negative and positive samples are already separated enough and no further optimization is required. When $\\theta_{a,p} \\leq \\theta_{a,n} + \\alpha$, $max(\\theta_{a,n} - \\theta_{a,p} + \\alpha , 0) = \\theta_{a,n} - \\theta_{a,p} + \\alpha$, and gradient descent on $L_{triplet}$ will minimize $\\theta_{a,n}$ and maximize $\\theta_{a,p}$. Given that the range of the cosine similarity scores are [-1, 1], let's empirically set $\\alpha=1$ in our experiment.\n",
    "\n",
    "Note that the difference between our loss and the loss in the illustration comes from the use of the **distance measure** $d(\\cdot)$ instead of a **similarity measure** in the illustration. If we use a distance measure here (e.g. Euclidean distance), then the loss needs to be modified accordingly.\n",
    "\n",
    "Now let's implement this triplet loss and add it to the original cross entropy loss. You need to write a new data loading function as we need one random positive sample and one negative sample for each of the anchor input in the training set. You can do something like:\n",
    "- Sample another batch of data for the triplet loss:\n",
    "    - Sample two speaker indices (e.g. speaker 2 and speaker 6).\n",
    "    - Sample two utterances from the first speaker (e.g speaker 2) to form the anchor input and the positive sample, and sample one utterance from the second speaker (speaker 6) to form the negative sample.\n",
    "    - Generate the speaker embeddings for the selected utterances, calculate cosine similarity and the triplet loss.\n",
    "\n",
    "Note that the validation stage also makes use of the randomly sampled utterances for triplet loss. If you want to use a fixed set of triplet input samples, you can predefine a set of speaker and utterance indices shared by all epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    15/   63 batches | ms/batch 372.44 | triplet loss 0.7626 |\n",
      "| epoch   1 |    30/   63 batches | ms/batch 378.46 | triplet loss 0.6842 |\n",
      "| epoch   1 |    45/   63 batches | ms/batch 377.93 | triplet loss 0.6255 |\n",
      "| epoch   1 |    60/   63 batches | ms/batch 376.96 | triplet loss 0.6457 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   1 | time: 23.67s | triplet loss 0.6406 |\n",
      "    | end of validation epoch   1 | time:  3.22s | triplet loss 0.5436 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   2 |    15/   63 batches | ms/batch 389.32 | triplet loss 0.6535 |\n",
      "| epoch   2 |    30/   63 batches | ms/batch 387.67 | triplet loss 0.5929 |\n",
      "| epoch   2 |    45/   63 batches | ms/batch 402.12 | triplet loss 0.5561 |\n",
      "| epoch   2 |    60/   63 batches | ms/batch 394.90 | triplet loss 0.5419 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   2 | time: 24.77s | triplet loss 0.5402 |\n",
      "    | end of validation epoch   2 | time:  3.42s | triplet loss 0.4244 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   3 |    15/   63 batches | ms/batch 383.17 | triplet loss 0.4089 |\n",
      "| epoch   3 |    30/   63 batches | ms/batch 389.22 | triplet loss 0.4719 |\n",
      "| epoch   3 |    45/   63 batches | ms/batch 388.46 | triplet loss 0.4441 |\n",
      "| epoch   3 |    60/   63 batches | ms/batch 388.70 | triplet loss 0.4808 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   3 | time: 24.44s | triplet loss 0.4790 |\n",
      "    | end of validation epoch   3 | time:  3.39s | triplet loss 0.4863 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   4 |    15/   63 batches | ms/batch 417.43 | triplet loss 0.5065 |\n",
      "| epoch   4 |    30/   63 batches | ms/batch 404.61 | triplet loss 0.4833 |\n",
      "| epoch   4 |    45/   63 batches | ms/batch 402.65 | triplet loss 0.4722 |\n",
      "| epoch   4 |    60/   63 batches | ms/batch 400.50 | triplet loss 0.4461 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   4 | time: 25.18s | triplet loss 0.4547 |\n",
      "    | end of validation epoch   4 | time:  3.75s | triplet loss 0.4218 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   5 |    15/   63 batches | ms/batch 362.77 | triplet loss 0.5309 |\n",
      "| epoch   5 |    30/   63 batches | ms/batch 368.40 | triplet loss 0.5214 |\n",
      "| epoch   5 |    45/   63 batches | ms/batch 359.48 | triplet loss 0.4778 |\n",
      "| epoch   5 |    60/   63 batches | ms/batch 359.59 | triplet loss 0.4562 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   5 | time: 22.70s | triplet loss 0.4677 |\n",
      "    | end of validation epoch   5 | time:  3.24s | triplet loss 0.3697 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   6 |    15/   63 batches | ms/batch 372.93 | triplet loss 0.4153 |\n",
      "| epoch   6 |    30/   63 batches | ms/batch 361.51 | triplet loss 0.4195 |\n",
      "| epoch   6 |    45/   63 batches | ms/batch 375.69 | triplet loss 0.4313 |\n",
      "| epoch   6 |    60/   63 batches | ms/batch 369.79 | triplet loss 0.4060 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   6 | time: 23.26s | triplet loss 0.4052 |\n",
      "    | end of validation epoch   6 | time:  3.03s | triplet loss 0.3770 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   7 |    15/   63 batches | ms/batch 322.95 | triplet loss 0.4480 |\n",
      "| epoch   7 |    30/   63 batches | ms/batch 323.37 | triplet loss 0.4467 |\n",
      "| epoch   7 |    45/   63 batches | ms/batch 331.67 | triplet loss 0.4328 |\n",
      "| epoch   7 |    60/   63 batches | ms/batch 341.05 | triplet loss 0.4207 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   7 | time: 21.44s | triplet loss 0.4173 |\n",
      "    | end of validation epoch   7 | time:  2.71s | triplet loss 0.5060 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   8 |    15/   63 batches | ms/batch 396.77 | triplet loss 0.4108 |\n",
      "| epoch   8 |    30/   63 batches | ms/batch 381.98 | triplet loss 0.4312 |\n",
      "| epoch   8 |    45/   63 batches | ms/batch 381.45 | triplet loss 0.4674 |\n",
      "| epoch   8 |    60/   63 batches | ms/batch 384.16 | triplet loss 0.4604 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   8 | time: 24.00s | triplet loss 0.4525 |\n",
      "    | end of validation epoch   8 | time:  3.67s | triplet loss 0.4617 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch   9 |    15/   63 batches | ms/batch 355.51 | triplet loss 0.4071 |\n",
      "| epoch   9 |    30/   63 batches | ms/batch 384.82 | triplet loss 0.3913 |\n",
      "| epoch   9 |    45/   63 batches | ms/batch 386.19 | triplet loss 0.3977 |\n",
      "| epoch   9 |    60/   63 batches | ms/batch 393.95 | triplet loss 0.4186 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch   9 | time: 24.83s | triplet loss 0.4114 |\n",
      "    | end of validation epoch   9 | time:  3.74s | triplet loss 0.4395 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  10 |    15/   63 batches | ms/batch 389.35 | triplet loss 0.3884 |\n",
      "| epoch  10 |    30/   63 batches | ms/batch 364.77 | triplet loss 0.4018 |\n",
      "| epoch  10 |    45/   63 batches | ms/batch 358.01 | triplet loss 0.4093 |\n",
      "| epoch  10 |    60/   63 batches | ms/batch 370.24 | triplet loss 0.4378 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  10 | time: 23.36s | triplet loss 0.4320 |\n",
      "    | end of validation epoch  10 | time:  3.04s | triplet loss 0.4610 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  11 |    15/   63 batches | ms/batch 370.72 | triplet loss 0.3782 |\n",
      "| epoch  11 |    30/   63 batches | ms/batch 364.63 | triplet loss 0.3789 |\n",
      "| epoch  11 |    45/   63 batches | ms/batch 362.67 | triplet loss 0.4094 |\n",
      "| epoch  11 |    60/   63 batches | ms/batch 369.99 | triplet loss 0.3933 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  11 | time: 23.24s | triplet loss 0.4000 |\n",
      "    | end of validation epoch  11 | time:  3.55s | triplet loss 0.4273 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  12 |    15/   63 batches | ms/batch 381.67 | triplet loss 0.4280 |\n",
      "| epoch  12 |    30/   63 batches | ms/batch 382.95 | triplet loss 0.4470 |\n",
      "| epoch  12 |    45/   63 batches | ms/batch 380.89 | triplet loss 0.4332 |\n",
      "| epoch  12 |    60/   63 batches | ms/batch 370.35 | triplet loss 0.4094 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  12 | time: 23.25s | triplet loss 0.4083 |\n",
      "    | end of validation epoch  12 | time:  3.00s | triplet loss 0.4214 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  13 |    15/   63 batches | ms/batch 354.81 | triplet loss 0.3745 |\n",
      "| epoch  13 |    30/   63 batches | ms/batch 360.71 | triplet loss 0.3580 |\n",
      "| epoch  13 |    45/   63 batches | ms/batch 357.34 | triplet loss 0.3660 |\n",
      "| epoch  13 |    60/   63 batches | ms/batch 367.91 | triplet loss 0.3602 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  13 | time: 23.20s | triplet loss 0.3647 |\n",
      "    | end of validation epoch  13 | time:  3.86s | triplet loss 0.4174 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  14 |    15/   63 batches | ms/batch 401.73 | triplet loss 0.3966 |\n",
      "| epoch  14 |    30/   63 batches | ms/batch 406.87 | triplet loss 0.3711 |\n",
      "| epoch  14 |    45/   63 batches | ms/batch 397.24 | triplet loss 0.3654 |\n",
      "| epoch  14 |    60/   63 batches | ms/batch 394.17 | triplet loss 0.3695 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  14 | time: 24.73s | triplet loss 0.3712 |\n",
      "    | end of validation epoch  14 | time:  3.49s | triplet loss 0.3971 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  15 |    15/   63 batches | ms/batch 427.98 | triplet loss 0.4170 |\n",
      "| epoch  15 |    30/   63 batches | ms/batch 391.09 | triplet loss 0.4135 |\n",
      "| epoch  15 |    45/   63 batches | ms/batch 384.79 | triplet loss 0.4110 |\n",
      "| epoch  15 |    60/   63 batches | ms/batch 381.13 | triplet loss 0.4101 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  15 | time: 24.06s | triplet loss 0.4021 |\n",
      "    | end of validation epoch  15 | time:  3.07s | triplet loss 0.3932 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  16 |    15/   63 batches | ms/batch 370.48 | triplet loss 0.3693 |\n",
      "| epoch  16 |    30/   63 batches | ms/batch 356.36 | triplet loss 0.4022 |\n",
      "| epoch  16 |    45/   63 batches | ms/batch 358.85 | triplet loss 0.3961 |\n",
      "| epoch  16 |    60/   63 batches | ms/batch 353.56 | triplet loss 0.4076 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  16 | time: 22.35s | triplet loss 0.4056 |\n",
      "    | end of validation epoch  16 | time:  3.06s | triplet loss 0.4103 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  17 |    15/   63 batches | ms/batch 324.44 | triplet loss 0.3353 |\n",
      "| epoch  17 |    30/   63 batches | ms/batch 329.27 | triplet loss 0.3319 |\n",
      "| epoch  17 |    45/   63 batches | ms/batch 355.38 | triplet loss 0.3383 |\n",
      "| epoch  17 |    60/   63 batches | ms/batch 359.48 | triplet loss 0.3573 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  17 | time: 22.65s | triplet loss 0.3579 |\n",
      "    | end of validation epoch  17 | time:  3.30s | triplet loss 0.3954 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  18 |    15/   63 batches | ms/batch 392.36 | triplet loss 0.3017 |\n",
      "| epoch  18 |    30/   63 batches | ms/batch 377.80 | triplet loss 0.3502 |\n",
      "| epoch  18 |    45/   63 batches | ms/batch 378.62 | triplet loss 0.3592 |\n",
      "| epoch  18 |    60/   63 batches | ms/batch 370.31 | triplet loss 0.3614 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  18 | time: 23.31s | triplet loss 0.3667 |\n",
      "    | end of validation epoch  18 | time:  3.42s | triplet loss 0.4185 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  19 |    15/   63 batches | ms/batch 400.29 | triplet loss 0.4000 |\n",
      "| epoch  19 |    30/   63 batches | ms/batch 392.09 | triplet loss 0.3679 |\n",
      "| epoch  19 |    45/   63 batches | ms/batch 389.66 | triplet loss 0.3606 |\n",
      "| epoch  19 |    60/   63 batches | ms/batch 389.61 | triplet loss 0.3818 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  19 | time: 24.55s | triplet loss 0.3847 |\n",
      "    | end of validation epoch  19 | time:  2.89s | triplet loss 0.4523 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  20 |    15/   63 batches | ms/batch 356.24 | triplet loss 0.3862 |\n",
      "| epoch  20 |    30/   63 batches | ms/batch 368.93 | triplet loss 0.3874 |\n",
      "| epoch  20 |    45/   63 batches | ms/batch 351.08 | triplet loss 0.3873 |\n",
      "| epoch  20 |    60/   63 batches | ms/batch 340.10 | triplet loss 0.3851 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  20 | time: 21.32s | triplet loss 0.3830 |\n",
      "    | end of validation epoch  20 | time:  2.61s | triplet loss 0.4403 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  21 |    15/   63 batches | ms/batch 324.39 | triplet loss 0.3281 |\n",
      "| epoch  21 |    30/   63 batches | ms/batch 316.18 | triplet loss 0.3804 |\n",
      "| epoch  21 |    45/   63 batches | ms/batch 330.89 | triplet loss 0.3671 |\n",
      "| epoch  21 |    60/   63 batches | ms/batch 340.20 | triplet loss 0.3583 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  21 | time: 21.41s | triplet loss 0.3555 |\n",
      "    | end of validation epoch  21 | time:  2.66s | triplet loss 0.2795 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  22 |    15/   63 batches | ms/batch 349.98 | triplet loss 0.3632 |\n",
      "| epoch  22 |    30/   63 batches | ms/batch 368.04 | triplet loss 0.3634 |\n",
      "| epoch  22 |    45/   63 batches | ms/batch 369.38 | triplet loss 0.3383 |\n",
      "| epoch  22 |    60/   63 batches | ms/batch 370.57 | triplet loss 0.3332 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  22 | time: 23.33s | triplet loss 0.3310 |\n",
      "    | end of validation epoch  22 | time:  3.18s | triplet loss 0.3914 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  23 |    15/   63 batches | ms/batch 366.70 | triplet loss 0.4407 |\n",
      "| epoch  23 |    30/   63 batches | ms/batch 361.44 | triplet loss 0.4202 |\n",
      "| epoch  23 |    45/   63 batches | ms/batch 358.22 | triplet loss 0.4137 |\n",
      "| epoch  23 |    60/   63 batches | ms/batch 362.80 | triplet loss 0.3925 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  23 | time: 22.76s | triplet loss 0.3925 |\n",
      "    | end of validation epoch  23 | time:  3.11s | triplet loss 0.4651 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  24 |    15/   63 batches | ms/batch 370.87 | triplet loss 0.3793 |\n",
      "| epoch  24 |    30/   63 batches | ms/batch 385.60 | triplet loss 0.3482 |\n",
      "| epoch  24 |    45/   63 batches | ms/batch 391.95 | triplet loss 0.3259 |\n",
      "| epoch  24 |    60/   63 batches | ms/batch 391.90 | triplet loss 0.3266 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  24 | time: 24.75s | triplet loss 0.3338 |\n",
      "    | end of validation epoch  24 | time:  3.42s | triplet loss 0.4469 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  25 |    15/   63 batches | ms/batch 393.71 | triplet loss 0.3103 |\n",
      "| epoch  25 |    30/   63 batches | ms/batch 382.90 | triplet loss 0.3231 |\n",
      "| epoch  25 |    45/   63 batches | ms/batch 373.87 | triplet loss 0.3183 |\n",
      "| epoch  25 |    60/   63 batches | ms/batch 358.44 | triplet loss 0.3072 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  25 | time: 22.46s | triplet loss 0.3089 |\n",
      "    | end of validation epoch  25 | time:  2.64s | triplet loss 0.2958 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  26 |    15/   63 batches | ms/batch 352.31 | triplet loss 0.3420 |\n",
      "| epoch  26 |    30/   63 batches | ms/batch 364.67 | triplet loss 0.3315 |\n",
      "| epoch  26 |    45/   63 batches | ms/batch 362.02 | triplet loss 0.2997 |\n",
      "| epoch  26 |    60/   63 batches | ms/batch 380.24 | triplet loss 0.2999 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  26 | time: 24.05s | triplet loss 0.2963 |\n",
      "    | end of validation epoch  26 | time:  3.89s | triplet loss 0.3601 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  27 |    15/   63 batches | ms/batch 409.74 | triplet loss 0.2686 |\n",
      "| epoch  27 |    30/   63 batches | ms/batch 420.07 | triplet loss 0.3769 |\n",
      "| epoch  27 |    45/   63 batches | ms/batch 410.50 | triplet loss 0.3729 |\n",
      "| epoch  27 |    60/   63 batches | ms/batch 398.83 | triplet loss 0.3709 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  27 | time: 24.87s | triplet loss 0.3721 |\n",
      "    | end of validation epoch  27 | time:  2.81s | triplet loss 0.3661 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  28 |    15/   63 batches | ms/batch 415.46 | triplet loss 0.3170 |\n",
      "| epoch  28 |    30/   63 batches | ms/batch 381.03 | triplet loss 0.3237 |\n",
      "| epoch  28 |    45/   63 batches | ms/batch 355.48 | triplet loss 0.3368 |\n",
      "| epoch  28 |    60/   63 batches | ms/batch 349.38 | triplet loss 0.3567 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  28 | time: 22.00s | triplet loss 0.3577 |\n",
      "    | end of validation epoch  28 | time:  3.73s | triplet loss 0.4219 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  29 |    15/   63 batches | ms/batch 391.27 | triplet loss 0.2972 |\n",
      "| epoch  29 |    30/   63 batches | ms/batch 370.71 | triplet loss 0.3233 |\n",
      "| epoch  29 |    45/   63 batches | ms/batch 378.68 | triplet loss 0.3548 |\n",
      "| epoch  29 |    60/   63 batches | ms/batch 381.83 | triplet loss 0.3551 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  29 | time: 23.95s | triplet loss 0.3536 |\n",
      "    | end of validation epoch  29 | time:  3.05s | triplet loss 0.4012 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  30 |    15/   63 batches | ms/batch 401.38 | triplet loss 0.3421 |\n",
      "| epoch  30 |    30/   63 batches | ms/batch 375.19 | triplet loss 0.3506 |\n",
      "| epoch  30 |    45/   63 batches | ms/batch 361.02 | triplet loss 0.3375 |\n",
      "| epoch  30 |    60/   63 batches | ms/batch 357.71 | triplet loss 0.3210 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  30 | time: 22.50s | triplet loss 0.3247 |\n",
      "    | end of validation epoch  30 | time:  2.90s | triplet loss 0.4419 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  31 |    15/   63 batches | ms/batch 352.77 | triplet loss 0.2522 |\n",
      "| epoch  31 |    30/   63 batches | ms/batch 343.34 | triplet loss 0.2467 |\n",
      "| epoch  31 |    45/   63 batches | ms/batch 357.30 | triplet loss 0.2578 |\n",
      "| epoch  31 |    60/   63 batches | ms/batch 364.94 | triplet loss 0.2766 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  31 | time: 22.98s | triplet loss 0.2834 |\n",
      "    | end of validation epoch  31 | time:  2.90s | triplet loss 0.4080 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  32 |    15/   63 batches | ms/batch 324.19 | triplet loss 0.4163 |\n",
      "| epoch  32 |    30/   63 batches | ms/batch 334.99 | triplet loss 0.3895 |\n",
      "| epoch  32 |    45/   63 batches | ms/batch 337.20 | triplet loss 0.3531 |\n",
      "| epoch  32 |    60/   63 batches | ms/batch 337.12 | triplet loss 0.3518 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  32 | time: 21.24s | triplet loss 0.3581 |\n",
      "    | end of validation epoch  32 | time:  2.83s | triplet loss 0.4745 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  33 |    15/   63 batches | ms/batch 344.44 | triplet loss 0.3294 |\n",
      "| epoch  33 |    30/   63 batches | ms/batch 344.22 | triplet loss 0.3038 |\n",
      "| epoch  33 |    45/   63 batches | ms/batch 343.36 | triplet loss 0.2956 |\n",
      "| epoch  33 |    60/   63 batches | ms/batch 339.81 | triplet loss 0.2980 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  33 | time: 21.37s | triplet loss 0.3045 |\n",
      "    | end of validation epoch  33 | time:  2.75s | triplet loss 0.3726 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  34 |    15/   63 batches | ms/batch 339.53 | triplet loss 0.2533 |\n",
      "| epoch  34 |    30/   63 batches | ms/batch 347.24 | triplet loss 0.2462 |\n",
      "| epoch  34 |    45/   63 batches | ms/batch 337.01 | triplet loss 0.2852 |\n",
      "| epoch  34 |    60/   63 batches | ms/batch 335.34 | triplet loss 0.2847 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  34 | time: 21.12s | triplet loss 0.2974 |\n",
      "    | end of validation epoch  34 | time:  2.81s | triplet loss 0.3874 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  35 |    15/   63 batches | ms/batch 344.86 | triplet loss 0.3374 |\n",
      "| epoch  35 |    30/   63 batches | ms/batch 333.44 | triplet loss 0.3217 |\n",
      "| epoch  35 |    45/   63 batches | ms/batch 339.34 | triplet loss 0.3093 |\n",
      "| epoch  35 |    60/   63 batches | ms/batch 338.69 | triplet loss 0.3154 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  35 | time: 21.38s | triplet loss 0.3122 |\n",
      "    | end of validation epoch  35 | time:  2.81s | triplet loss 0.4101 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  36 |    15/   63 batches | ms/batch 361.06 | triplet loss 0.3665 |\n",
      "| epoch  36 |    30/   63 batches | ms/batch 349.12 | triplet loss 0.3418 |\n",
      "| epoch  36 |    45/   63 batches | ms/batch 341.49 | triplet loss 0.3371 |\n",
      "| epoch  36 |    60/   63 batches | ms/batch 339.07 | triplet loss 0.3121 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  36 | time: 21.35s | triplet loss 0.3175 |\n",
      "    | end of validation epoch  36 | time:  2.80s | triplet loss 0.3854 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  37 |    15/   63 batches | ms/batch 338.88 | triplet loss 0.2599 |\n",
      "| epoch  37 |    30/   63 batches | ms/batch 347.20 | triplet loss 0.3053 |\n",
      "| epoch  37 |    45/   63 batches | ms/batch 349.60 | triplet loss 0.3235 |\n",
      "| epoch  37 |    60/   63 batches | ms/batch 347.10 | triplet loss 0.3246 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  37 | time: 21.89s | triplet loss 0.3207 |\n",
      "    | end of validation epoch  37 | time:  2.87s | triplet loss 0.3802 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  38 |    15/   63 batches | ms/batch 338.10 | triplet loss 0.3216 |\n",
      "| epoch  38 |    30/   63 batches | ms/batch 338.34 | triplet loss 0.3306 |\n",
      "| epoch  38 |    45/   63 batches | ms/batch 340.60 | triplet loss 0.3422 |\n",
      "| epoch  38 |    60/   63 batches | ms/batch 337.61 | triplet loss 0.3308 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  38 | time: 21.25s | triplet loss 0.3243 |\n",
      "    | end of validation epoch  38 | time:  2.88s | triplet loss 0.4285 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  39 |    15/   63 batches | ms/batch 339.84 | triplet loss 0.2683 |\n",
      "| epoch  39 |    30/   63 batches | ms/batch 338.86 | triplet loss 0.2760 |\n",
      "| epoch  39 |    45/   63 batches | ms/batch 348.45 | triplet loss 0.2800 |\n",
      "| epoch  39 |    60/   63 batches | ms/batch 346.13 | triplet loss 0.2903 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  39 | time: 21.72s | triplet loss 0.2893 |\n",
      "    | end of validation epoch  39 | time:  2.73s | triplet loss 0.3417 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  40 |    15/   63 batches | ms/batch 321.67 | triplet loss 0.3369 |\n",
      "| epoch  40 |    30/   63 batches | ms/batch 340.09 | triplet loss 0.3088 |\n",
      "| epoch  40 |    45/   63 batches | ms/batch 341.09 | triplet loss 0.3110 |\n",
      "| epoch  40 |    60/   63 batches | ms/batch 336.27 | triplet loss 0.3069 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  40 | time: 21.27s | triplet loss 0.3159 |\n",
      "    | end of validation epoch  40 | time:  2.65s | triplet loss 0.4233 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  41 |    15/   63 batches | ms/batch 347.23 | triplet loss 0.2794 |\n",
      "| epoch  41 |    30/   63 batches | ms/batch 338.61 | triplet loss 0.2629 |\n",
      "| epoch  41 |    45/   63 batches | ms/batch 331.61 | triplet loss 0.2866 |\n",
      "| epoch  41 |    60/   63 batches | ms/batch 331.81 | triplet loss 0.2734 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  41 | time: 20.88s | triplet loss 0.2763 |\n",
      "    | end of validation epoch  41 | time:  2.94s | triplet loss 0.4178 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  42 |    15/   63 batches | ms/batch 362.27 | triplet loss 0.2267 |\n",
      "| epoch  42 |    30/   63 batches | ms/batch 369.40 | triplet loss 0.2579 |\n",
      "| epoch  42 |    45/   63 batches | ms/batch 370.68 | triplet loss 0.2869 |\n",
      "| epoch  42 |    60/   63 batches | ms/batch 364.91 | triplet loss 0.2908 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  42 | time: 22.96s | triplet loss 0.2838 |\n",
      "    | end of validation epoch  42 | time:  3.12s | triplet loss 0.3919 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  43 |    15/   63 batches | ms/batch 359.84 | triplet loss 0.2587 |\n",
      "| epoch  43 |    30/   63 batches | ms/batch 357.79 | triplet loss 0.2587 |\n",
      "| epoch  43 |    45/   63 batches | ms/batch 349.00 | triplet loss 0.2598 |\n",
      "| epoch  43 |    60/   63 batches | ms/batch 351.07 | triplet loss 0.2689 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  43 | time: 22.03s | triplet loss 0.2705 |\n",
      "    | end of validation epoch  43 | time:  2.82s | triplet loss 0.3788 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  44 |    15/   63 batches | ms/batch 331.49 | triplet loss 0.2641 |\n",
      "| epoch  44 |    30/   63 batches | ms/batch 324.48 | triplet loss 0.2881 |\n",
      "| epoch  44 |    45/   63 batches | ms/batch 336.95 | triplet loss 0.2877 |\n",
      "| epoch  44 |    60/   63 batches | ms/batch 335.78 | triplet loss 0.2692 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  44 | time: 21.08s | triplet loss 0.2685 |\n",
      "    | end of validation epoch  44 | time:  2.76s | triplet loss 0.3908 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  45 |    15/   63 batches | ms/batch 375.10 | triplet loss 0.3329 |\n",
      "| epoch  45 |    30/   63 batches | ms/batch 378.45 | triplet loss 0.3680 |\n",
      "| epoch  45 |    45/   63 batches | ms/batch 366.35 | triplet loss 0.3812 |\n",
      "| epoch  45 |    60/   63 batches | ms/batch 364.04 | triplet loss 0.3803 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  45 | time: 22.96s | triplet loss 0.3728 |\n",
      "    | end of validation epoch  45 | time:  3.01s | triplet loss 0.4368 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  46 |    15/   63 batches | ms/batch 357.16 | triplet loss 0.2998 |\n",
      "| epoch  46 |    30/   63 batches | ms/batch 353.34 | triplet loss 0.3019 |\n",
      "| epoch  46 |    45/   63 batches | ms/batch 340.69 | triplet loss 0.2816 |\n",
      "| epoch  46 |    60/   63 batches | ms/batch 334.56 | triplet loss 0.2818 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  46 | time: 20.99s | triplet loss 0.2843 |\n",
      "    | end of validation epoch  46 | time:  2.66s | triplet loss 0.3857 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  47 |    15/   63 batches | ms/batch 308.12 | triplet loss 0.2941 |\n",
      "| epoch  47 |    30/   63 batches | ms/batch 307.83 | triplet loss 0.2763 |\n",
      "| epoch  47 |    45/   63 batches | ms/batch 309.55 | triplet loss 0.2918 |\n",
      "| epoch  47 |    60/   63 batches | ms/batch 309.86 | triplet loss 0.2775 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  47 | time: 19.50s | triplet loss 0.2783 |\n",
      "    | end of validation epoch  47 | time:  2.69s | triplet loss 0.3938 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  48 |    15/   63 batches | ms/batch 307.47 | triplet loss 0.2765 |\n",
      "| epoch  48 |    30/   63 batches | ms/batch 308.93 | triplet loss 0.2823 |\n",
      "| epoch  48 |    45/   63 batches | ms/batch 310.82 | triplet loss 0.3181 |\n",
      "| epoch  48 |    60/   63 batches | ms/batch 310.09 | triplet loss 0.3102 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  48 | time: 19.51s | triplet loss 0.3196 |\n",
      "    | end of validation epoch  48 | time:  2.60s | triplet loss 0.4704 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  49 |    15/   63 batches | ms/batch 308.10 | triplet loss 0.3114 |\n",
      "| epoch  49 |    30/   63 batches | ms/batch 306.15 | triplet loss 0.3180 |\n",
      "| epoch  49 |    45/   63 batches | ms/batch 306.58 | triplet loss 0.2931 |\n",
      "| epoch  49 |    60/   63 batches | ms/batch 306.23 | triplet loss 0.2925 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  49 | time: 19.28s | triplet loss 0.2947 |\n",
      "    | end of validation epoch  49 | time:  2.60s | triplet loss 0.3999 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  50 |    15/   63 batches | ms/batch 301.07 | triplet loss 0.2426 |\n",
      "| epoch  50 |    30/   63 batches | ms/batch 311.51 | triplet loss 0.2682 |\n",
      "| epoch  50 |    45/   63 batches | ms/batch 315.99 | triplet loss 0.2660 |\n",
      "| epoch  50 |    60/   63 batches | ms/batch 322.96 | triplet loss 0.2590 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  50 | time: 20.39s | triplet loss 0.2521 |\n",
      "    | end of validation epoch  50 | time:  3.13s | triplet loss 0.2832 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  51 |    15/   63 batches | ms/batch 354.96 | triplet loss 0.2317 |\n",
      "| epoch  51 |    30/   63 batches | ms/batch 364.47 | triplet loss 0.2698 |\n",
      "| epoch  51 |    45/   63 batches | ms/batch 367.30 | triplet loss 0.2958 |\n",
      "| epoch  51 |    60/   63 batches | ms/batch 352.81 | triplet loss 0.3019 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  51 | time: 22.11s | triplet loss 0.2943 |\n",
      "    | end of validation epoch  51 | time:  2.61s | triplet loss 0.4106 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  52 |    15/   63 batches | ms/batch 301.34 | triplet loss 0.2300 |\n",
      "| epoch  52 |    30/   63 batches | ms/batch 304.17 | triplet loss 0.2447 |\n",
      "| epoch  52 |    45/   63 batches | ms/batch 303.38 | triplet loss 0.2933 |\n",
      "| epoch  52 |    60/   63 batches | ms/batch 303.11 | triplet loss 0.2983 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  52 | time: 19.09s | triplet loss 0.2918 |\n",
      "    | end of validation epoch  52 | time:  2.57s | triplet loss 0.3939 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  53 |    15/   63 batches | ms/batch 298.27 | triplet loss 0.2792 |\n",
      "| epoch  53 |    30/   63 batches | ms/batch 301.05 | triplet loss 0.2656 |\n",
      "| epoch  53 |    45/   63 batches | ms/batch 300.18 | triplet loss 0.2563 |\n",
      "| epoch  53 |    60/   63 batches | ms/batch 301.53 | triplet loss 0.2449 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  53 | time: 19.02s | triplet loss 0.2484 |\n",
      "    | end of validation epoch  53 | time:  2.61s | triplet loss 0.4106 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  54 |    15/   63 batches | ms/batch 307.20 | triplet loss 0.2465 |\n",
      "| epoch  54 |    30/   63 batches | ms/batch 302.94 | triplet loss 0.2524 |\n",
      "| epoch  54 |    45/   63 batches | ms/batch 302.07 | triplet loss 0.2546 |\n",
      "| epoch  54 |    60/   63 batches | ms/batch 301.77 | triplet loss 0.2365 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  54 | time: 19.08s | triplet loss 0.2340 |\n",
      "    | end of validation epoch  54 | time:  2.64s | triplet loss 0.4014 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  55 |    15/   63 batches | ms/batch 303.31 | triplet loss 0.2523 |\n",
      "| epoch  55 |    30/   63 batches | ms/batch 301.44 | triplet loss 0.2910 |\n",
      "| epoch  55 |    45/   63 batches | ms/batch 302.35 | triplet loss 0.3200 |\n",
      "| epoch  55 |    60/   63 batches | ms/batch 301.71 | triplet loss 0.3300 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  55 | time: 18.99s | triplet loss 0.3195 |\n",
      "    | end of validation epoch  55 | time:  2.56s | triplet loss 0.3141 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  56 |    15/   63 batches | ms/batch 302.37 | triplet loss 0.3740 |\n",
      "| epoch  56 |    30/   63 batches | ms/batch 301.11 | triplet loss 0.3157 |\n",
      "| epoch  56 |    45/   63 batches | ms/batch 300.99 | triplet loss 0.2818 |\n",
      "| epoch  56 |    60/   63 batches | ms/batch 300.83 | triplet loss 0.2682 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  56 | time: 18.95s | triplet loss 0.2717 |\n",
      "    | end of validation epoch  56 | time:  2.62s | triplet loss 0.3055 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  57 |    15/   63 batches | ms/batch 296.61 | triplet loss 0.2801 |\n",
      "| epoch  57 |    30/   63 batches | ms/batch 297.21 | triplet loss 0.2941 |\n",
      "| epoch  57 |    45/   63 batches | ms/batch 300.84 | triplet loss 0.2744 |\n",
      "| epoch  57 |    60/   63 batches | ms/batch 303.69 | triplet loss 0.2660 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  57 | time: 19.11s | triplet loss 0.2686 |\n",
      "    | end of validation epoch  57 | time:  2.60s | triplet loss 0.3145 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  58 |    15/   63 batches | ms/batch 297.66 | triplet loss 0.2390 |\n",
      "| epoch  58 |    30/   63 batches | ms/batch 300.39 | triplet loss 0.2686 |\n",
      "| epoch  58 |    45/   63 batches | ms/batch 299.51 | triplet loss 0.2609 |\n",
      "| epoch  58 |    60/   63 batches | ms/batch 299.20 | triplet loss 0.2591 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  58 | time: 18.88s | triplet loss 0.2550 |\n",
      "    | end of validation epoch  58 | time:  2.69s | triplet loss 0.3267 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  59 |    15/   63 batches | ms/batch 301.97 | triplet loss 0.2864 |\n",
      "| epoch  59 |    30/   63 batches | ms/batch 303.01 | triplet loss 0.2399 |\n",
      "| epoch  59 |    45/   63 batches | ms/batch 302.09 | triplet loss 0.2373 |\n",
      "| epoch  59 |    60/   63 batches | ms/batch 302.01 | triplet loss 0.2466 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  59 | time: 19.02s | triplet loss 0.2479 |\n",
      "    | end of validation epoch  59 | time:  2.59s | triplet loss 0.2914 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  60 |    15/   63 batches | ms/batch 301.63 | triplet loss 0.2586 |\n",
      "| epoch  60 |    30/   63 batches | ms/batch 304.67 | triplet loss 0.2692 |\n",
      "| epoch  60 |    45/   63 batches | ms/batch 306.05 | triplet loss 0.2679 |\n",
      "| epoch  60 |    60/   63 batches | ms/batch 304.88 | triplet loss 0.2730 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  60 | time: 19.19s | triplet loss 0.2784 |\n",
      "    | end of validation epoch  60 | time:  2.60s | triplet loss 0.3411 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  61 |    15/   63 batches | ms/batch 303.38 | triplet loss 0.2720 |\n",
      "| epoch  61 |    30/   63 batches | ms/batch 301.49 | triplet loss 0.2659 |\n",
      "| epoch  61 |    45/   63 batches | ms/batch 301.16 | triplet loss 0.2463 |\n",
      "| epoch  61 |    60/   63 batches | ms/batch 309.70 | triplet loss 0.2525 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  61 | time: 19.51s | triplet loss 0.2536 |\n",
      "    | end of validation epoch  61 | time:  2.56s | triplet loss 0.5015 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  62 |    15/   63 batches | ms/batch 316.16 | triplet loss 0.1978 |\n",
      "| epoch  62 |    30/   63 batches | ms/batch 311.45 | triplet loss 0.2107 |\n",
      "| epoch  62 |    45/   63 batches | ms/batch 308.24 | triplet loss 0.2425 |\n",
      "| epoch  62 |    60/   63 batches | ms/batch 307.08 | triplet loss 0.2435 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  62 | time: 19.31s | triplet loss 0.2410 |\n",
      "    | end of validation epoch  62 | time:  2.58s | triplet loss 0.4106 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  63 |    15/   63 batches | ms/batch 309.99 | triplet loss 0.2544 |\n",
      "| epoch  63 |    30/   63 batches | ms/batch 307.16 | triplet loss 0.2757 |\n",
      "| epoch  63 |    45/   63 batches | ms/batch 305.10 | triplet loss 0.2806 |\n",
      "| epoch  63 |    60/   63 batches | ms/batch 303.35 | triplet loss 0.2629 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  63 | time: 19.16s | triplet loss 0.2571 |\n",
      "    | end of validation epoch  63 | time:  2.57s | triplet loss 0.3396 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  64 |    15/   63 batches | ms/batch 298.71 | triplet loss 0.2079 |\n",
      "| epoch  64 |    30/   63 batches | ms/batch 300.17 | triplet loss 0.2359 |\n",
      "| epoch  64 |    45/   63 batches | ms/batch 299.66 | triplet loss 0.2250 |\n",
      "| epoch  64 |    60/   63 batches | ms/batch 300.30 | triplet loss 0.2281 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  64 | time: 18.95s | triplet loss 0.2281 |\n",
      "    | end of validation epoch  64 | time:  2.57s | triplet loss 0.2791 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  65 |    15/   63 batches | ms/batch 300.23 | triplet loss 0.2030 |\n",
      "| epoch  65 |    30/   63 batches | ms/batch 302.11 | triplet loss 0.2516 |\n",
      "| epoch  65 |    45/   63 batches | ms/batch 301.39 | triplet loss 0.2305 |\n",
      "| epoch  65 |    60/   63 batches | ms/batch 301.55 | triplet loss 0.2341 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  65 | time: 18.99s | triplet loss 0.2314 |\n",
      "    | end of validation epoch  65 | time:  2.70s | triplet loss 0.2995 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  66 |    15/   63 batches | ms/batch 301.85 | triplet loss 0.2426 |\n",
      "| epoch  66 |    30/   63 batches | ms/batch 301.80 | triplet loss 0.2438 |\n",
      "| epoch  66 |    45/   63 batches | ms/batch 300.85 | triplet loss 0.2441 |\n",
      "| epoch  66 |    60/   63 batches | ms/batch 300.63 | triplet loss 0.2411 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  66 | time: 18.94s | triplet loss 0.2370 |\n",
      "    | end of validation epoch  66 | time:  2.56s | triplet loss 0.3988 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  67 |    15/   63 batches | ms/batch 311.30 | triplet loss 0.1966 |\n",
      "| epoch  67 |    30/   63 batches | ms/batch 305.73 | triplet loss 0.2262 |\n",
      "| epoch  67 |    45/   63 batches | ms/batch 307.84 | triplet loss 0.2260 |\n",
      "| epoch  67 |    60/   63 batches | ms/batch 306.67 | triplet loss 0.2331 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  67 | time: 19.30s | triplet loss 0.2331 |\n",
      "    | end of validation epoch  67 | time:  2.57s | triplet loss 0.3678 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  68 |    15/   63 batches | ms/batch 298.25 | triplet loss 0.2149 |\n",
      "| epoch  68 |    30/   63 batches | ms/batch 297.39 | triplet loss 0.2424 |\n",
      "| epoch  68 |    45/   63 batches | ms/batch 299.17 | triplet loss 0.2798 |\n",
      "| epoch  68 |    60/   63 batches | ms/batch 303.42 | triplet loss 0.2836 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  68 | time: 19.11s | triplet loss 0.2892 |\n",
      "    | end of validation epoch  68 | time:  2.65s | triplet loss 0.3898 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  69 |    15/   63 batches | ms/batch 299.25 | triplet loss 0.2853 |\n",
      "| epoch  69 |    30/   63 batches | ms/batch 299.72 | triplet loss 0.3040 |\n",
      "| epoch  69 |    45/   63 batches | ms/batch 299.57 | triplet loss 0.3216 |\n",
      "| epoch  69 |    60/   63 batches | ms/batch 298.94 | triplet loss 0.3145 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  69 | time: 18.85s | triplet loss 0.3140 |\n",
      "    | end of validation epoch  69 | time:  2.57s | triplet loss 0.3021 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  70 |    15/   63 batches | ms/batch 305.16 | triplet loss 0.2868 |\n",
      "| epoch  70 |    30/   63 batches | ms/batch 304.69 | triplet loss 0.2722 |\n",
      "| epoch  70 |    45/   63 batches | ms/batch 307.20 | triplet loss 0.2543 |\n",
      "| epoch  70 |    60/   63 batches | ms/batch 305.73 | triplet loss 0.2478 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  70 | time: 19.25s | triplet loss 0.2493 |\n",
      "    | end of validation epoch  70 | time:  2.58s | triplet loss 0.3575 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  71 |    15/   63 batches | ms/batch 295.20 | triplet loss 0.2356 |\n",
      "| epoch  71 |    30/   63 batches | ms/batch 303.25 | triplet loss 0.2487 |\n",
      "| epoch  71 |    45/   63 batches | ms/batch 308.37 | triplet loss 0.2445 |\n",
      "| epoch  71 |    60/   63 batches | ms/batch 309.38 | triplet loss 0.2208 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  71 | time: 19.45s | triplet loss 0.2201 |\n",
      "    | end of validation epoch  71 | time:  2.55s | triplet loss 0.3112 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  72 |    15/   63 batches | ms/batch 300.62 | triplet loss 0.2805 |\n",
      "| epoch  72 |    30/   63 batches | ms/batch 300.77 | triplet loss 0.2340 |\n",
      "| epoch  72 |    45/   63 batches | ms/batch 300.13 | triplet loss 0.2275 |\n",
      "| epoch  72 |    60/   63 batches | ms/batch 300.30 | triplet loss 0.2239 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  72 | time: 18.91s | triplet loss 0.2209 |\n",
      "    | end of validation epoch  72 | time:  2.55s | triplet loss 0.5005 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  73 |    15/   63 batches | ms/batch 322.57 | triplet loss 0.3404 |\n",
      "| epoch  73 |    30/   63 batches | ms/batch 310.38 | triplet loss 0.3297 |\n",
      "| epoch  73 |    45/   63 batches | ms/batch 306.56 | triplet loss 0.3065 |\n",
      "| epoch  73 |    60/   63 batches | ms/batch 304.86 | triplet loss 0.2912 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  73 | time: 19.18s | triplet loss 0.2923 |\n",
      "    | end of validation epoch  73 | time:  2.51s | triplet loss 0.3435 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  74 |    15/   63 batches | ms/batch 302.38 | triplet loss 0.2326 |\n",
      "| epoch  74 |    30/   63 batches | ms/batch 307.45 | triplet loss 0.2276 |\n",
      "| epoch  74 |    45/   63 batches | ms/batch 308.25 | triplet loss 0.2498 |\n",
      "| epoch  74 |    60/   63 batches | ms/batch 305.82 | triplet loss 0.2431 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  74 | time: 19.26s | triplet loss 0.2418 |\n",
      "    | end of validation epoch  74 | time:  2.64s | triplet loss 0.3988 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  75 |    15/   63 batches | ms/batch 311.83 | triplet loss 0.2695 |\n",
      "| epoch  75 |    30/   63 batches | ms/batch 309.98 | triplet loss 0.2717 |\n",
      "| epoch  75 |    45/   63 batches | ms/batch 307.65 | triplet loss 0.2625 |\n",
      "| epoch  75 |    60/   63 batches | ms/batch 307.00 | triplet loss 0.2574 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  75 | time: 19.33s | triplet loss 0.2530 |\n",
      "    | end of validation epoch  75 | time:  2.76s | triplet loss 0.3424 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  76 |    15/   63 batches | ms/batch 304.98 | triplet loss 0.2591 |\n",
      "| epoch  76 |    30/   63 batches | ms/batch 302.48 | triplet loss 0.2691 |\n",
      "| epoch  76 |    45/   63 batches | ms/batch 305.98 | triplet loss 0.2483 |\n",
      "| epoch  76 |    60/   63 batches | ms/batch 304.80 | triplet loss 0.2570 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  76 | time: 19.19s | triplet loss 0.2503 |\n",
      "    | end of validation epoch  76 | time:  2.61s | triplet loss 0.4075 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  77 |    15/   63 batches | ms/batch 306.58 | triplet loss 0.2396 |\n",
      "| epoch  77 |    30/   63 batches | ms/batch 304.75 | triplet loss 0.2471 |\n",
      "| epoch  77 |    45/   63 batches | ms/batch 301.52 | triplet loss 0.2396 |\n",
      "| epoch  77 |    60/   63 batches | ms/batch 301.04 | triplet loss 0.2489 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  77 | time: 18.95s | triplet loss 0.2473 |\n",
      "    | end of validation epoch  77 | time:  2.60s | triplet loss 0.3191 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  78 |    15/   63 batches | ms/batch 305.36 | triplet loss 0.2487 |\n",
      "| epoch  78 |    30/   63 batches | ms/batch 303.08 | triplet loss 0.2483 |\n",
      "| epoch  78 |    45/   63 batches | ms/batch 302.20 | triplet loss 0.2233 |\n",
      "| epoch  78 |    60/   63 batches | ms/batch 301.10 | triplet loss 0.2299 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  78 | time: 18.96s | triplet loss 0.2229 |\n",
      "    | end of validation epoch  78 | time:  2.53s | triplet loss 0.2852 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  79 |    15/   63 batches | ms/batch 299.18 | triplet loss 0.2467 |\n",
      "| epoch  79 |    30/   63 batches | ms/batch 300.80 | triplet loss 0.2071 |\n",
      "| epoch  79 |    45/   63 batches | ms/batch 300.00 | triplet loss 0.2021 |\n",
      "| epoch  79 |    60/   63 batches | ms/batch 301.53 | triplet loss 0.2125 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  79 | time: 19.01s | triplet loss 0.2178 |\n",
      "    | end of validation epoch  79 | time:  2.57s | triplet loss 0.3315 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  80 |    15/   63 batches | ms/batch 297.05 | triplet loss 0.2546 |\n",
      "| epoch  80 |    30/   63 batches | ms/batch 296.45 | triplet loss 0.3159 |\n",
      "| epoch  80 |    45/   63 batches | ms/batch 296.49 | triplet loss 0.3175 |\n",
      "| epoch  80 |    60/   63 batches | ms/batch 296.72 | triplet loss 0.3148 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  80 | time: 18.68s | triplet loss 0.3088 |\n",
      "    | end of validation epoch  80 | time:  2.55s | triplet loss 0.3377 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  81 |    15/   63 batches | ms/batch 298.95 | triplet loss 0.2250 |\n",
      "| epoch  81 |    30/   63 batches | ms/batch 299.52 | triplet loss 0.2560 |\n",
      "| epoch  81 |    45/   63 batches | ms/batch 300.20 | triplet loss 0.2508 |\n",
      "| epoch  81 |    60/   63 batches | ms/batch 299.23 | triplet loss 0.2441 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  81 | time: 18.86s | triplet loss 0.2463 |\n",
      "    | end of validation epoch  81 | time:  2.50s | triplet loss 0.3964 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  82 |    15/   63 batches | ms/batch 296.05 | triplet loss 0.2401 |\n",
      "| epoch  82 |    30/   63 batches | ms/batch 297.08 | triplet loss 0.2337 |\n",
      "| epoch  82 |    45/   63 batches | ms/batch 299.48 | triplet loss 0.2494 |\n",
      "| epoch  82 |    60/   63 batches | ms/batch 300.19 | triplet loss 0.2448 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  82 | time: 18.90s | triplet loss 0.2413 |\n",
      "    | end of validation epoch  82 | time:  2.57s | triplet loss 0.3440 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  83 |    15/   63 batches | ms/batch 296.38 | triplet loss 0.2227 |\n",
      "| epoch  83 |    30/   63 batches | ms/batch 297.33 | triplet loss 0.2262 |\n",
      "| epoch  83 |    45/   63 batches | ms/batch 297.46 | triplet loss 0.2377 |\n",
      "| epoch  83 |    60/   63 batches | ms/batch 297.83 | triplet loss 0.2403 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  83 | time: 18.77s | triplet loss 0.2385 |\n",
      "    | end of validation epoch  83 | time:  2.53s | triplet loss 0.3438 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  84 |    15/   63 batches | ms/batch 296.53 | triplet loss 0.1892 |\n",
      "| epoch  84 |    30/   63 batches | ms/batch 297.28 | triplet loss 0.2234 |\n",
      "| epoch  84 |    45/   63 batches | ms/batch 296.56 | triplet loss 0.2315 |\n",
      "| epoch  84 |    60/   63 batches | ms/batch 296.61 | triplet loss 0.2439 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  84 | time: 18.68s | triplet loss 0.2404 |\n",
      "    | end of validation epoch  84 | time:  2.56s | triplet loss 0.3367 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  85 |    15/   63 batches | ms/batch 296.23 | triplet loss 0.1732 |\n",
      "| epoch  85 |    30/   63 batches | ms/batch 298.69 | triplet loss 0.1930 |\n",
      "| epoch  85 |    45/   63 batches | ms/batch 301.37 | triplet loss 0.1873 |\n",
      "| epoch  85 |    60/   63 batches | ms/batch 305.43 | triplet loss 0.1979 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  85 | time: 19.26s | triplet loss 0.1979 |\n",
      "    | end of validation epoch  85 | time:  2.62s | triplet loss 0.3927 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best training model found.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  86 |    15/   63 batches | ms/batch 302.92 | triplet loss 0.1848 |\n",
      "| epoch  86 |    30/   63 batches | ms/batch 306.78 | triplet loss 0.2080 |\n",
      "| epoch  86 |    45/   63 batches | ms/batch 309.01 | triplet loss 0.2075 |\n",
      "| epoch  86 |    60/   63 batches | ms/batch 309.43 | triplet loss 0.2079 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  86 | time: 19.44s | triplet loss 0.2077 |\n",
      "    | end of validation epoch  86 | time:  2.61s | triplet loss 0.3098 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  87 |    15/   63 batches | ms/batch 297.78 | triplet loss 0.2380 |\n",
      "| epoch  87 |    30/   63 batches | ms/batch 296.52 | triplet loss 0.2221 |\n",
      "| epoch  87 |    45/   63 batches | ms/batch 296.44 | triplet loss 0.2116 |\n",
      "| epoch  87 |    60/   63 batches | ms/batch 296.60 | triplet loss 0.2296 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  87 | time: 18.68s | triplet loss 0.2312 |\n",
      "    | end of validation epoch  87 | time:  2.54s | triplet loss 0.3706 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  88 |    15/   63 batches | ms/batch 296.20 | triplet loss 0.2064 |\n",
      "| epoch  88 |    30/   63 batches | ms/batch 301.96 | triplet loss 0.1955 |\n",
      "| epoch  88 |    45/   63 batches | ms/batch 309.15 | triplet loss 0.2101 |\n",
      "| epoch  88 |    60/   63 batches | ms/batch 325.07 | triplet loss 0.2011 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  88 | time: 20.60s | triplet loss 0.2130 |\n",
      "    | end of validation epoch  88 | time:  3.04s | triplet loss 0.2924 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  89 |    15/   63 batches | ms/batch 362.71 | triplet loss 0.2634 |\n",
      "| epoch  89 |    30/   63 batches | ms/batch 357.97 | triplet loss 0.2464 |\n",
      "| epoch  89 |    45/   63 batches | ms/batch 367.39 | triplet loss 0.2630 |\n",
      "| epoch  89 |    60/   63 batches | ms/batch 368.28 | triplet loss 0.2687 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  89 | time: 23.15s | triplet loss 0.2647 |\n",
      "    | end of validation epoch  89 | time:  2.54s | triplet loss 0.3525 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  90 |    15/   63 batches | ms/batch 377.19 | triplet loss 0.2347 |\n",
      "| epoch  90 |    30/   63 batches | ms/batch 352.89 | triplet loss 0.2175 |\n",
      "| epoch  90 |    45/   63 batches | ms/batch 362.36 | triplet loss 0.2081 |\n",
      "| epoch  90 |    60/   63 batches | ms/batch 363.96 | triplet loss 0.2208 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  90 | time: 22.96s | triplet loss 0.2247 |\n",
      "    | end of validation epoch  90 | time:  3.46s | triplet loss 0.3153 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  91 |    15/   63 batches | ms/batch 392.51 | triplet loss 0.2736 |\n",
      "| epoch  91 |    30/   63 batches | ms/batch 391.15 | triplet loss 0.2712 |\n",
      "| epoch  91 |    45/   63 batches | ms/batch 379.21 | triplet loss 0.2762 |\n",
      "| epoch  91 |    60/   63 batches | ms/batch 360.53 | triplet loss 0.2637 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  91 | time: 22.57s | triplet loss 0.2609 |\n",
      "    | end of validation epoch  91 | time:  2.66s | triplet loss 0.3547 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  92 |    15/   63 batches | ms/batch 302.20 | triplet loss 0.2239 |\n",
      "| epoch  92 |    30/   63 batches | ms/batch 302.87 | triplet loss 0.2149 |\n",
      "| epoch  92 |    45/   63 batches | ms/batch 303.31 | triplet loss 0.2243 |\n",
      "| epoch  92 |    60/   63 batches | ms/batch 305.84 | triplet loss 0.2241 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  92 | time: 19.25s | triplet loss 0.2200 |\n",
      "    | end of validation epoch  92 | time:  2.58s | triplet loss 0.3753 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  93 |    15/   63 batches | ms/batch 307.92 | triplet loss 0.2973 |\n",
      "| epoch  93 |    30/   63 batches | ms/batch 305.82 | triplet loss 0.2642 |\n",
      "| epoch  93 |    45/   63 batches | ms/batch 305.74 | triplet loss 0.2557 |\n",
      "| epoch  93 |    60/   63 batches | ms/batch 304.55 | triplet loss 0.2415 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  93 | time: 19.16s | triplet loss 0.2386 |\n",
      "    | end of validation epoch  93 | time:  2.57s | triplet loss 0.2923 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  94 |    15/   63 batches | ms/batch 298.93 | triplet loss 0.2648 |\n",
      "| epoch  94 |    30/   63 batches | ms/batch 299.14 | triplet loss 0.2323 |\n",
      "| epoch  94 |    45/   63 batches | ms/batch 300.12 | triplet loss 0.2376 |\n",
      "| epoch  94 |    60/   63 batches | ms/batch 300.57 | triplet loss 0.2403 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  94 | time: 18.94s | triplet loss 0.2443 |\n",
      "    | end of validation epoch  94 | time:  2.57s | triplet loss 0.3410 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  95 |    15/   63 batches | ms/batch 304.91 | triplet loss 0.2501 |\n",
      "| epoch  95 |    30/   63 batches | ms/batch 302.58 | triplet loss 0.2736 |\n",
      "| epoch  95 |    45/   63 batches | ms/batch 302.44 | triplet loss 0.2770 |\n",
      "| epoch  95 |    60/   63 batches | ms/batch 301.88 | triplet loss 0.2620 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  95 | time: 19.09s | triplet loss 0.2666 |\n",
      "    | end of validation epoch  95 | time:  2.60s | triplet loss 0.4013 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  96 |    15/   63 batches | ms/batch 300.62 | triplet loss 0.2188 |\n",
      "| epoch  96 |    30/   63 batches | ms/batch 313.71 | triplet loss 0.2546 |\n",
      "| epoch  96 |    45/   63 batches | ms/batch 326.94 | triplet loss 0.2314 |\n",
      "| epoch  96 |    60/   63 batches | ms/batch 321.46 | triplet loss 0.2420 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  96 | time: 20.19s | triplet loss 0.2404 |\n",
      "    | end of validation epoch  96 | time:  2.57s | triplet loss 0.2948 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  97 |    15/   63 batches | ms/batch 300.65 | triplet loss 0.2211 |\n",
      "| epoch  97 |    30/   63 batches | ms/batch 301.62 | triplet loss 0.1998 |\n",
      "| epoch  97 |    45/   63 batches | ms/batch 300.60 | triplet loss 0.1988 |\n",
      "| epoch  97 |    60/   63 batches | ms/batch 301.67 | triplet loss 0.2151 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  97 | time: 19.00s | triplet loss 0.2165 |\n",
      "    | end of validation epoch  97 | time:  2.55s | triplet loss 0.2524 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "      Best validation model found and saved.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  98 |    15/   63 batches | ms/batch 320.64 | triplet loss 0.2108 |\n",
      "| epoch  98 |    30/   63 batches | ms/batch 310.66 | triplet loss 0.2333 |\n",
      "| epoch  98 |    45/   63 batches | ms/batch 310.89 | triplet loss 0.2458 |\n",
      "| epoch  98 |    60/   63 batches | ms/batch 308.78 | triplet loss 0.2398 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  98 | time: 19.44s | triplet loss 0.2457 |\n",
      "    | end of validation epoch  98 | time:  2.71s | triplet loss 0.3449 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch  99 |    15/   63 batches | ms/batch 303.22 | triplet loss 0.1568 |\n",
      "| epoch  99 |    30/   63 batches | ms/batch 304.06 | triplet loss 0.1795 |\n",
      "| epoch  99 |    45/   63 batches | ms/batch 302.96 | triplet loss 0.2055 |\n",
      "| epoch  99 |    60/   63 batches | ms/batch 308.36 | triplet loss 0.2139 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch  99 | time: 19.55s | triplet loss 0.2151 |\n",
      "    | end of validation epoch  99 | time:  2.89s | triplet loss 0.4018 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "| epoch 100 |    15/   63 batches | ms/batch 324.25 | triplet loss 0.2543 |\n",
      "| epoch 100 |    30/   63 batches | ms/batch 315.22 | triplet loss 0.2418 |\n",
      "| epoch 100 |    45/   63 batches | ms/batch 321.05 | triplet loss 0.2263 |\n",
      "| epoch 100 |    60/   63 batches | ms/batch 325.38 | triplet loss 0.2280 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "    | end of training epoch 100 | time: 20.48s | triplet loss 0.2262 |\n",
      "    | end of validation epoch 100 | time:  2.74s | triplet loss 0.4582 |\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# TODO: train another network with triplet loss\n",
    "\n",
    "model_siamese = AlexNet()\n",
    "\n",
    "def sample_triplet(mode='train'):\n",
    "    \n",
    "    if mode == 'train':\n",
    "        sample_num = 10\n",
    "        dataset = train_audio\n",
    "    elif mode == 'val':\n",
    "        sample_num = 3\n",
    "        dataset = val_audio\n",
    "        \n",
    "    # sample two speaker indices\n",
    "    spk_idx = np.random.choice(50, 2, replace=False)\n",
    "    \n",
    "    # for the first speaker, sample two utterance indices\n",
    "    # each speaker has 10 utterance for training\n",
    "    anchor_idx, positive_idx = np.random.choice(sample_num, 2, replace=False)\n",
    "        \n",
    "    # for the second speaker, sample one utterance index\n",
    "    negative_idx = np.random.choice(sample_num)\n",
    "        \n",
    "    # load the utterances\n",
    "    anchor_utterance = dataset[spk_idx[0]*sample_num+anchor_idx]\n",
    "    positive_utterance = dataset[spk_idx[0]*sample_num+positive_idx]\n",
    "    negative_utterance = dataset[spk_idx[1]*sample_num+negative_idx]\n",
    "        \n",
    "    # calculate STFT\n",
    "    anchor_spec = librosa.stft(anchor_utterance.astype(np.float32), n_fft=512, hop_length=256)\n",
    "    positive_spec = librosa.stft(positive_utterance.astype(np.float32), n_fft=512, hop_length=256)\n",
    "    negative_spec = librosa.stft(negative_utterance.astype(np.float32), n_fft=512, hop_length=256)\n",
    "    anchor_spec = torch.from_numpy(np.abs(anchor_spec))\n",
    "    positive_spec = torch.from_numpy(np.abs(positive_spec))\n",
    "    negative_spec = torch.from_numpy(np.abs(negative_spec))\n",
    "    \n",
    "    return anchor_spec, positive_spec, negative_spec\n",
    "\n",
    "# triplet loss\n",
    "def triplet_loss(theta_ap, theta_an, alpha=1):\n",
    "    # theta_ap shape: (batch,)\n",
    "    # theta_an shape: (batch,)\n",
    "    \n",
    "    loss = F.relu(theta_an - theta_ap + alpha)\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "def train(model, epoch, versatile=True):\n",
    "    start_time = time.time()\n",
    "    model = model.train()  # set the model to training mode. Always do this before you start training!\n",
    "    train_loss = 0.\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # triplet loss\n",
    "        batch_anchor_spec = []\n",
    "        batch_positive_spec = []\n",
    "        batch_negative_spec = []\n",
    "        for j in range(batch_size):\n",
    "            anchor_spec, positive_spec, negative_spec = sample_triplet(mode='train')\n",
    "            batch_anchor_spec.append(anchor_spec.unsqueeze(0))\n",
    "            batch_positive_spec.append(positive_spec.unsqueeze(0))\n",
    "            batch_negative_spec.append(negative_spec.unsqueeze(0))\n",
    "            \n",
    "        batch_anchor_spec = torch.cat(batch_anchor_spec, 0)\n",
    "        batch_positive_spec = torch.cat(batch_positive_spec, 0)\n",
    "        batch_negative_spec = torch.cat(batch_negative_spec, 0)\n",
    "        \n",
    "        # get the embeddings\n",
    "        _ , anchor_embedding = model(batch_anchor_spec.unsqueeze(1))\n",
    "        _ , positive_embedding = model(batch_positive_spec.unsqueeze(1))\n",
    "        _ , negative_embedding = model(batch_negative_spec.unsqueeze(1))\n",
    "        \n",
    "        # calculate cosine similarity scores\n",
    "        theta_ap = (anchor_embedding * positive_embedding).sum(1)\n",
    "        theta_an = (anchor_embedding * negative_embedding).sum(1)\n",
    "        \n",
    "        # triplet loss\n",
    "        loss = triplet_loss(theta_ap, theta_an)\n",
    "        \n",
    "        # automatically calculate the backward pass\n",
    "        loss.backward()\n",
    "        # perform the actual backpropagation\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data.item()\n",
    "        \n",
    "        # OPTIONAL: you can print the training progress \n",
    "        if versatile:\n",
    "            if (batch_idx+1) % log_step == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | triplet loss {:5.4f} |'.format(\n",
    "                    epoch, batch_idx+1, len(train_loader),\n",
    "                    elapsed * 1000 / (batch_idx+1), \n",
    "                    train_loss / (batch_idx+1)\n",
    "                    ))\n",
    "    \n",
    "    train_loss /= (batch_idx+1)\n",
    "    print('-' * 99)\n",
    "    print('    | end of training epoch {:3d} | time: {:5.2f}s | triplet loss {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), train_loss))\n",
    "    \n",
    "    return train_loss\n",
    "\n",
    "def validate(model, epoch):\n",
    "    start_time = time.time()\n",
    "    model = model.eval()  # set the model to evaluation mode. Always do this during validation or test phase!\n",
    "    validation_loss = 0\n",
    "    \n",
    "    # load batch data\n",
    "    for batch_idx, data in enumerate(validation_loader):\n",
    "        \n",
    "        # you don't need to calculate the backward pass and the gradients during validation\n",
    "        # so you can call torch.no_grad() to only calculate the forward pass to save time and memory\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            # triplet loss\n",
    "            batch_anchor_spec = []\n",
    "            batch_positive_spec = []\n",
    "            batch_negative_spec = []\n",
    "            for j in range(batch_size):\n",
    "                anchor_spec, positive_spec, negative_spec = sample_triplet(mode='val')\n",
    "                batch_anchor_spec.append(anchor_spec.unsqueeze(0))\n",
    "                batch_positive_spec.append(positive_spec.unsqueeze(0))\n",
    "                batch_negative_spec.append(negative_spec.unsqueeze(0))\n",
    "\n",
    "            batch_anchor_spec = torch.cat(batch_anchor_spec, 0)\n",
    "            batch_positive_spec = torch.cat(batch_positive_spec, 0)\n",
    "            batch_negative_spec = torch.cat(batch_negative_spec, 0)\n",
    "\n",
    "            # get the embeddings\n",
    "            _ , anchor_embedding = model(batch_anchor_spec.unsqueeze(1))\n",
    "            _ , positive_embedding = model(batch_positive_spec.unsqueeze(1))\n",
    "            _ , negative_embedding = model(batch_negative_spec.unsqueeze(1))\n",
    "\n",
    "            # calculate cosine similarity scores\n",
    "            theta_ap = (anchor_embedding * positive_embedding).sum(1)\n",
    "            theta_an = (anchor_embedding * negative_embedding).sum(1)\n",
    "\n",
    "            # triplet loss\n",
    "            loss = triplet_loss(theta_ap, theta_an)\n",
    "            \n",
    "            validation_loss += loss.data.numpy()\n",
    "        \n",
    "    validation_loss = validation_loss / (batch_idx+1)\n",
    "    print('    | end of validation epoch {:3d} | time: {:5.2f}s | triplet loss {:5.4f} |'.format(\n",
    "            epoch, (time.time() - start_time), validation_loss))\n",
    "    print('-' * 99)\n",
    "    \n",
    "    return validation_loss\n",
    "\n",
    "\n",
    "total_epoch = 100  # train the model for 100 epochs\n",
    "model_save = 'best_AlexNet_siamese.pt'  # path to save the best validation model\n",
    "optimizer = optim.Adam(model_siamese.parameters(), lr=1e-3)\n",
    "\n",
    "# main function\n",
    "\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    training_loss.append(train(model_siamese, epoch))\n",
    "    validation_loss.append(validate(model_siamese, epoch))\n",
    "    \n",
    "    if training_loss[-1] == np.min(training_loss):\n",
    "        print('      Best training model found.')\n",
    "    if validation_loss[-1] == np.min(validation_loss):\n",
    "        # save current best model on validation set\n",
    "        with open(model_save, 'wb') as f:\n",
    "            torch.save(model_siamese.state_dict(), f)\n",
    "            print('      Best validation model found and saved.')\n",
    "    \n",
    "    print('-' * 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you finish the model training, evaluate your model by calculating the EER for the verification test. Compare the EER between the multiclass classification model and thie siamese network with triplet loss. What do you find? How can you improve the performance of the model with the worse performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 50)\n",
      "Overall EER: 18.00%; decision threshold for similarity scores: 0.69\n"
     ]
    }
   ],
   "source": [
    "# TODO: evaluate the model, calculate EER as above, and make the comparison\n",
    "# remember to re-generate the average embeddings and the test embeddings\n",
    "\n",
    "# load the best model on validation set\n",
    "model_siamese.load_state_dict(torch.load('best_AlexNet_siamese.pt'))\n",
    "model_siamese.eval()\n",
    "\n",
    "all_embedding = []\n",
    "\n",
    "for i in range(len(train_audio)):\n",
    "    this_spec = librosa.stft(train_audio[i].astype(np.float32), n_fft=512, hop_length=256)\n",
    "    spec = torch.from_numpy(np.abs(this_spec))  # only use the magnitude spectrogram\n",
    "    \n",
    "    _, output_embedding = model_siamese(spec.unsqueeze(0).unsqueeze(1))\n",
    "    all_embedding.append(output_embedding)\n",
    "    \n",
    "all_embedding = torch.cat(all_embedding, 0).data.numpy()  # 500, 256\n",
    "average_embedding = np.mean(all_embedding.reshape(50, 10, -1), 1)  # 50, 256\n",
    "\n",
    "test_embedding = []\n",
    "\n",
    "for i in range(len(test_audio)):\n",
    "    this_spec = librosa.stft(test_audio[i].astype(np.float32), n_fft=512, hop_length=256)\n",
    "    spec = torch.from_numpy(np.abs(this_spec))  # only use the magnitude spectrogram\n",
    "    \n",
    "    output, output_embedding = model_siamese(spec.unsqueeze(0).unsqueeze(1))\n",
    "    test_embedding.append(output_embedding)\n",
    "    \n",
    "test_embedding = torch.cat(test_embedding, 0).data.numpy()  # 50, 256\n",
    "\n",
    "cos_sim = np.dot(test_embedding, average_embedding.T) / np.sqrt(np.sum(average_embedding**2, 1)[:,np.newaxis] + 1e-6)\n",
    "print(cos_sim.shape)\n",
    "eer, decision_threshold = EER(label, cos_sim.reshape(-1))\n",
    "print('Overall EER: {:.2f}%; decision threshold for similarity scores: {:.2f}'.format(eer*100, decision_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: enter your observations and comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: Conventional Speaker Recognition Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to neural networks, the most popular methods for speaker recognition are [**Joint Factor Analysis (JFA)**](https://www.researchgate.net/profile/Patrick_Kenny/publication/228922248_Joint_factor_analysis_of_speaker_and_session_variability_Theory_and_algorithms/links/00b49519e4362ef144000000/Joint-factor-analysis-of-speaker-and-session-variability-Theory-and-algorithms.pdf) and [**i-vector**](https://ieeexplore.ieee.org/abstract/document/5545402). \n",
    "\n",
    "The general idea of these two methods is straightforward. Both methods attempt to present *all possible speech utterances* in a shared embedding space (like a Gaussian Mixture Model) - you can imagine that it is a generative model such that by assigning proper sampling strategies you can obtain speech utterances from different speakers. The embedding space contains everything including phonemes, pitch, and speaker identity, and for each utterance you have different weights applied to the embeddings of each of these characteristics, and this embedding space requires a training step with available data (like when we train the networks). The weight for the speaker identity characteristic is used as the speaker embeddings that distinguish different speakers. During model training, similar approaches can be applied like what we do here - we can still train a classifier such that speaker embeddings from the utterances from the same speaker are close to each other, and embeddings from the utterances from the different speakers are far from each other. This is typically done by Bayesian approaches.\n",
    "\n",
    "Compared with the neural network approach, the embedding space is replaced by the mapping function defined by the neural network - we assume that the neural network itself is a strong enough feature extractor that only cares about the speaker identity and *removes* all the other characteristics in a given utterance. JFA and i-vector are both linear models and their performance can be constrained due to the model capacity, while the nonlinearity of the neural networks can better model the large amount of data and have a better representation and feature extraction power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: Scoring Method\n",
    "\n",
    "We are using the most simple way of scoring with cosine similarity score. However, the most widely-used method for scoring is [***probabilistic linear discriminant analysis (PLDA)***](https://towardsdatascience.com/probabilistic-linear-discriminant-analysis-plda-explained-253b5effb96). PLDA is a probablistic framework that estimates the probability that a given speaker embedding belongs to the target speaker's speaker embeddings (like a GMM). For the details of how to apply PLDA scoring, you can check [this paper](https://ieeexplore.ieee.org/document/7078610) or [this implementation](https://github.com/RicherMans/PLDA)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
